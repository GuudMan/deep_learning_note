#### 1. seq2seq中的attention机制

seq2seq模型的输入是序列(如单词、字母或图像特征等），输出是另外一个序列，模型训练过程可表示为如下过程：

![img](images/1686706224318-e76fd1c2-f617-4b79-9515-c99ab23fc318.gif)

在机器翻译中，输入的序列是单词，一个一个的处理，输出的同样是一系列的单词。

![img](images/1686706420515-353a5d66-7f85-4674-9143-3b5bc05cc781.gif)

##### 2、深度理解模型内部机制

在模型内部由encoder和decoder组成。encoder处理每一个输入序列，将输入序列转为一个向量(称为context)。处理完所有的输入序列，encoder将context发送给decoder，decoder开始逐渐项生成输出序列。

![img](images/1686706955804-0b3c0966-38ef-4bcb-af27-94b7b053ac33.gif)

机器翻译过程的原理也类似

![img](images/1686707044373-9722a349-036b-4e91-88b8-79b46aa70391.gif)

在机器翻译案例中context是一个向量，encoder和decoder都是循环神经网络。

![img](images/1686707194781-9476dc85-52fb-4e6d-83d6-8aa6f19fe1a7.png)	

设置模型时可设置context的大小，在encoder的RNN中它是隐藏单元的一个数字。上图中展示的是大小为4的向量，但在实际应用中这个向量大小可以更大，如256/512/1024.



设计时， RNN每一步都会接收两个输入：一个输入(在encoder中，它是输入句子中的一个单词)和一个隐藏状态。但是这个输入单词需要表征为一个向量。我们将单词转为向量的过程称为word embedding，词嵌入算法。这些由单词变成的矢量空间会捕捉单词的大量词义和语义信息。

![img](images/1686707683506-1b582840-6392-4e28-af44-40c8a893adbb.png)

在处理之前需要将输入单词转为向量。这个转变过程是通过word embedding词嵌入算法完成的。我们可以使用预训练的词嵌入或者在我们的数据集上训练我们自己的词嵌入。通常词嵌入向量大小是200或300，这里为了简化使用大小为4的向量。



下面介绍主要的向量/张量，回顾一下RNN的原理，建立一个可视化语言去描述这个模型。

![img](images/1686708256871-ba246ca9-8c5a-45fe-b8fa-a5b9e7e6b938.gif)

下一个RNN将第二个输入向量和隐藏状态1去创建这个时间步的输出。

在下面的可视化过程中，encoder或decoder的每个脉冲都是RNN处理它的输入并生成该时间步的输出。因为encoder和decoder都是RNN，RNN的每个时间步都做同样的处理，根据当前的输入和上一次输入去更新它的隐藏状态。

![img](images/1686708838603-13fa8488-d19a-4b07-b5c7-ac3b3010dc83.gif)



decoder同样保留了隐藏状态，这个隐藏状态从当前时间步到下一个时间步。目前没有这个图形中把它可视化，因为现在关注的模型的主要部分。



下面看一下另外一种可视化sequence2sequence模型的方法。这个动画可以更好的描述模型的静态图形。这里称为“unrolled"视图。这里不显示解码器，而是显示它在每个时间步长的副本。这样可以看到每个时间步骤的输入和输出。

![img](images/1686709353816-3e7717a9-744a-451f-bf38-c5a9b691bcdc.gif)





context向量被视为这类模型的瓶颈。在面临长序列时会是模型的挑战。[Bahdanau et al., 2014](https://arxiv.org/abs/1409.0473) and [Luong et al., 2015](https://arxiv.org/abs/1508.04025).提出了解决方法。论文中介绍并提炼了一种“注意力”机制，该机制极大的提升了机器翻译系统的质量。注意力机制允许模型根据需要关注输入序列的相关部分。

![img](images/1686709676084-418dc1cf-a17b-4d07-8286-9cb9b879f8d3.png)

在第7步中，在生成英文翻译前，attention机制允许decoder关注"etudiant"(法语，表示“学生”）。这种放大输入序列相关部分的信号的能力使得模型比没有注意力的模型产生更好的结果。



在高度抽象的层面上继续查看这个模型。注意力模型与经典的sequence-to-sequence主要有如下两个方面的不同。

1、encoder会发送更多的数据给decoder。encoder将所有的隐藏状态发送给decoder而不是将encoder阶段最后的隐藏状态发送给decoder。

![img](images/1686711470406-b6b08861-4071-48d6-9350-489c00a07950.gif)

2、注意力decoder在生成输出前会有一个额外的步骤。为了关注与当前decode时间步相关的部分，decoder会做如下的处理

- 1、查看它接收的一系列encoder隐藏状态——每个encoder隐藏状态都会与输入序列中的特定单词相关。
- 2、给一个隐藏状态打分
- 3、将隐藏状态与softmax得分相乘，这会放大得分更高的隐藏状态，缩小得分低的隐藏状态。

![img](images/1686711937735-4db24a9b-455f-47e2-973d-b2f293cba2cd.gif)



打分过程在decoder阶段每个时间步都会执行。

在下面的动画过程中归纳以上所有的步骤，看看attention过程是如何完成的。

- 1、attention decoder接收<END>标记的嵌入和初级decoder的隐藏状态。
- 2、RNN处理输入，产生输出和一个新的隐藏状态向量(h4)。输出被丢弃了。
- 3、attention step: 使用encoder的隐藏状态和h4向量计算当前时间步的context向量（C4)
- 4、将h4和C4拼接成一个向量。
- 5、将4中的拼接完成的向量输入到前馈神经网络中(与模型共同训练的)
- 6、前馈神经网络的输出代表的是当前时间步的输出单词
- 7、下一个时间步重复以上步骤。



![img](images/1686712998588-547a0e8f-37cc-4c57-a353-f2631296abb8.gif)



下面是另外一种方法，该方法用来观测我们在每个decoder步骤中关注输入句子的哪一部分：

![img](images/1686713346988-3321dbdb-63a8-4187-8098-002edb3b4c01.gif)

请注意，模型并不会毫无意识的将输出的第一个单词与输入的第一个单词对齐，实际上它在模型训练阶段学会了如何对齐该语言对中的单词。



#### 2. The Illustrated transformer

上面的文章中介绍了Attention——一个在深度学习模型中无处不在的方法。Attention是一个可以提升神经机器翻译性能的概念。在这篇文章中会探讨The Transformer——一个利用注意力机制来提升模型训练速度的模型。在特定任务重，这个Transformer模型已超过Google神经机器翻译模型。最大的益处来自于Transformer如何进行并行化。事实上，谷歌云推荐使用The Transformer作为参考模型来使用他们的云TPU产品。因此，让我们试着把这个模型拆开，看看它的功能如何。

##### 2.1. 简介

将这个模型看成一个简单的黑盒子。在机器翻译应用中，一种语言的句子作为输入，输出是翻译成的另外一个句子。

![img](images/1686723963414-7902cefd-bdab-4b28-8d48-7252df99282f.png)

打开内部的机制，我们会看到encoding模块，decoding模块以及他们之间的连接。

![img](images/1686724078303-b2b2ba4e-50fd-4161-89fb-dfa8a1ee05c6.png)

encoding模块是一系列堆叠的encoder(论文中堆叠了6个)， decoding模块同样也是堆叠了相同数字的decoder.

![img](images/1686724207883-5aeb1ab4-4656-4de1-8247-8ef957d5d72f.png)

结构中的encoder都不一样(或者说它们并没共享权重).每一个都可以分解为两个层。

![img](images/1686724363273-16ab46dd-94b7-4695-ae89-d49d34454bee.png)

encoder的输入首先会经过self-attention层——它可以帮助encoder在编码特定单词时关注句子中的其他单词。

self-attention层的输出会进入前馈神经网络。完全相同的前馈网络独立的应用在每个地方。

deocder也有这些层，但在它们之间有一个attention层帮助decoder关注输入句子的相关部分。

![img](images/1686725409993-e8966248-8127-4be4-9bfb-54b2d22317cb.png)

##### 2.2. tensor可视化

现在知道了模型的主要组成部分，下面开始关注一些向量/张量以及在这些组件之间如何将训练模型的输入转变为一个输出的。

正如一般的nlp应用情况一样，现在开始使用词嵌入算法将输入单词变成向量。

![img](images/1686725726821-4837e51e-1c71-47f6-adda-30feb25cf1a2.png)

词嵌入只会出现在encoder的最底层，所有encoder都有一个共同的抽象概念即它们接收大小为512维度的向量列表——在底部encoder中输入是词嵌入，在其他encoder中的输入是其他正下方encoder的输出。向量列表的大小是一个可以设置的超参数——一般上它是我们训练数据中最长句子的长度。

在输入序列中嵌入单词后，每个单词都会流经encoder中的两层。

![img](images/1686726179864-b3cc48e4-d450-4b78-989d-63bba1fee2f3.png)

这里会介绍Transformer中的一个关键特性，即每个位置的单词在encoder中流经它自己的路径。在self-attention层存在着依赖关系。在前馈网络层并没有这种依赖性，因此，各种路径在流经前馈层前可以并行执行。

##### 2.3. encoding层

上文已提到，encoder会接收很多向量作为输入。这些向量会经过两个处理：首先经过self-attention层和前馈神经网络层，然后会将输出发送到下一个encoder。

![img](images/1686726796793-a9e68731-c065-414c-a2ce-6ee984ad222c.png)



##### 2.4. self-attention

假设我们想翻译下面的句子：

“The animal didn't cross the street beause it was too tired”

句子中的"it"代表什么呢？是代表street还是代表animal？对人类来说非常简单，但对算法来说往往没有这么容易。

当模型处理单词"it"时， self-attention会将“it"与“animal”联系起来。

当模型处理每个单词时(输入序列中的每个位置)， self-attention使它能够在输入序列中的其他位置寻找线索来帮助这个模型更好的完成单词的编码。

如果你熟悉RNNs, 想想维持一个隐藏状态是如何让RNN把它以前处理过的词/向量的表示与它现在处理的词/向量结合起来的。self-attention是Transformer用来将其他相关单词的理解烘托到我们当前正在处理的单词的方法。

![img](images/1686727953967-60371df7-3a99-4d2a-bcc0-81d998d6adf5.png)

当我们在5号编码器（堆栈中最顶端的编码器)中对"it"进行编码时，部分注意力机制集中在”The animal"上，并将其他部分标准烘托到"it"的编码中。

##### 2.5. self-attention细节

我们先来看看如何使用向量来计算自我注意力，然后再继续看它使用矩阵的实现方式。

**计算self-attention的第一步**是从每个encoder的输入向量(也就是词嵌入)创建三个向量。因此对每个单词，我们会创建一个Query向量，一个Key向量，一个Value向量。这些向量是通过将词嵌入与我们在训练过程中训练的三个矩阵相乘而产生的。

可以发现这些新的向量比词嵌入向量更小。它们有64维，而词嵌入和encoder的输入和输出都是512维。他们不一定要更小，这是一个架构选择，以使multi-head attention的计算（大部分）恒定。

![img](images/1686728957200-304f69ed-5201-4b14-89af-8083f45da7f7.png)

Multiplying x1 by the WQ weight matrix produces q1, the "query" vector associated with that word. We end up creating a "query", a "key", and a "value" projection of each word in the input sentence.



"query" ，key"和"value"向量是什么？

他们是对计算和理解attention有好处的一种抽象。



**计算self-attention的第二步是打分：**假设我们正在计算例子中第一个单词"Thinking"的self-attention. 我们需要对输入句子中的每个词与这个词进行打分。这个分数决定了当我们要对输入句子中的某一位置的单词进行编码时，要对输入句子中的其他部分给予多大的关注。

这个分数通过将query向量与每个打分单词的key向量进行点乘。所以如果我们正在处理位置1的单词self-attention, 第一个分数是q1与k1的点乘，第二个分数是q1与k2的点乘。

![img](images/1686729851781-fb873c2c-a04d-4006-bd93-4cf49b4a7014.png)

**第三步和第四步：**是将分数除以8(key向量维度开根号， 论文中是64， 所以这里是除以8，这可以得到更加稳定的梯度。这里可能有其他可能的值，但这是默认的). 然后将结果发送给softmax处理，Softmax将分数归一，所以它们都是正数，加起来是1。

![img](images/1686730228360-f409a3ea-c263-4c7f-8d9f-9362474878c6.png)



softmax分数决定了每个词在这个位置上的表达量。显然，这个位置上的词会有最高的softmax得分，但有时关注另一个与当前词相关的词是很有用的。

**第5步：** 将value向量与softmax得分相乘(为了将它们相加), 这一步的直觉是保持关注单词的完整价值和过滤掉不相关单词(例如，将0.0001与它们相乘)。



**第6步：**将所有加权的value向量向乘，这就产生了self-attention层在这个位置的输出。

![img](images/1686730933443-9b791b26-00b1-4f84-a50d-9c57a20b72de.png)

上面总结了self-attention的计算过程。self-attention的输出向量会发送到前馈神经网络层。然而，实际实现过程中，这种计算是以矩阵形式进行的，以便更快处理。

##### 2.6. self-attention的矩阵计算

**第一步：**第一步是计算Query/Key和value矩阵。将词嵌入包装成矩阵X，然后与我们训练的权重矩阵(WQ, WK, WV)相乘。

![img](images/1686731314350-cf22ae34-c6bf-4eef-ba08-4d46545c3083.png)

图中，X矩阵中的每一行与输入句子相对应，我们可以看到词嵌入向量(大小为512， 图中的4个小方格)与q/k/v向量(64维， 图中的3个小方格)的不同。

**最后：** 我们再处理矩阵，所以我们可以把第二到第六步浓缩在一个公式中，以计算自我注意层的输出。

![img](images/1686731679169-cafef8af-104e-4b77-89c6-35419b82b4b4.png)

##### 2.7. 多头注意力机制 multi head

通过增加一个“多头”注意力机制来完善self-attention层，可在如下两个方面提升attention层的性能。

- \1. 扩展了模型的能力，可以使模型关不不同的位置。在上面的例子汇总，z1包含其他每一种encoding中的一点点，但它可能被实际的单词本身所限制。
- \2. 它赋予attention层多个"表征子空间"。下面我们会看到，在multi-heah attention中有多组Query、Key、Value权重矩阵而不是只有一个(transformer使用8个attention head, 所以我们为每个encoders/decoers提供了8组权重矩阵)， 每一组权重矩阵都是随机初始化的。训练之后，每一组权重矩阵都被用于将输入embedding(或来自下级的encoder、decoder向量)投射到不同的表征子空间。

![img](images/1686794022608-b9194c5c-ff94-4e57-b107-4acf8c828884.jpeg)

对于multi-head attention，我们为每个头保持单独的K/Q/V, 从而产生不同的K/Q/V矩阵。与之前的做法类似，我们用X乘以WQ/WK/WV矩阵来产生Q/K/V矩阵。

如果我们像上面总结的那样去计算self-attention，对不同的权重矩阵仅仅需要计算8次最终得到8个不同的z矩阵。

![img](images/1686794729671-e259d0f4-828e-4f4f-bffb-e3316baa6f47.png)



这会给我们带来一些挑战，前馈神经网络层并不希望得到8个矩阵——它只希望得到一个句子(一个单词一个句子)。所以我们需要用某种方法将8个矩阵拼接成一个单独的矩阵。

如何做呢？ 我们首先将这些矩阵拼接然后再乘以一个额外的权重矩阵WO。

![img](images/1686794967056-448166b6-6be1-4004-a87a-1bcf59716345.png)



这几乎是multi-head attention的全部内容。我意识到它有太多的矩阵了，我尝试将这些矩阵放在一个可视化的地方。

![img](images/1686795124112-573a4832-5e6a-48be-b0fd-fae8638da03e.png)

既然我们已经了解了attention heads，现在我们来看一下， 在我们提供的例句中，当我们对单词"it"进行编码时不同的attention head会关注哪些地方。

![img](images/1686795495449-edf66402-b8af-4ce9-bfb8-9a3c2fb025e7.png)

当我们在对单词"it"编码时，一个注意力头大部分关注在"the animal"， 而其他的关注在"tired"——某种意义上，模型对单词"it"的表征在“animal”和"tired"上都有一些。



如果我们将所有的注意力头都放在一个照片中，将会变得很难去解释。

![img](images/1686795870231-f0596424-f7a9-47c3-812b-6b1b6e0a4d75.png)

##### 2.8. 使用位置编码表征序列的顺序

到目前为止， 我们所描述的模型中还缺少一样东西，那就是对输入序列单词顺序的说明。

为了解决这个问题，transformer在每个输入embedding中加入了一个向量。这些向量遵循一个模型学习的特特定模式，这个模式可以帮助决定每个单词的位置，或者在序列中不但单词之间的距离。这里的直觉是，一旦嵌入向量被投射到Q/K/V向量中，在点乘attention中，将这些值添加到embedding中，就能提供嵌入向量之间有意义的距离。

![img](images/1686796999645-5fc4166e-8497-46dc-9303-0d844265e3ea.png)



假设词嵌入有4维， 实际的位置编码可能是这样的。

![img](images/1686797083178-37e071bc-3f60-4cd1-a7cb-4fba94e02ba1.png)



这个模式是怎么的呢？

在下面的图中，每一行都与位置编码向量有关。所以第一行就是我们在输入序列中添加到embedding中的向量。每行包含512个值——每个值都在-1和1之间。我们将其进行颜色编码，所以这个模式是可见的。

![img](images/1686797316482-b2cda17f-2be9-4d4b-8be7-ff30e69b7abf.png)

一个位置编码的真实案例，20个字(行)的词嵌入大小为512(列)。可以看到它们从中间开始分成两半。这是因为左半部分的值是由一个函数产生的(使用的是sine函数)，而右半部分使用的是另外一个方程（使用的是cosine)。然后将它们串联起来，形成每个位置编码向量。

它不仅仅是位置编码函数，它能够扩展到未见过的长度的序列(例如，如果我们的训练模型柏要求翻译一个比我们训练集中的任何句子都长的句子）。



2020年7月更新：上面显示的位置编码是来自Tranformer2Transformer的实现。论文中显示的方法略有不同，它不是直接串联，而是将两个信号交织在一起。下图显示了它的样子。

![img](images/1686798724740-0e0b83ec-2a2b-4b9c-a125-0070ccf72e9e.png)

##### 2.9. 残差结构

encoder结构中有一个细节是每个encoder中的子层(self attention)中都有一个残差连接，下面展示的是layr-normalization步骤。

![img](images/1686799148925-369c51be-1e29-4a16-8f8a-247cf4f7aa22.png)

如果把向量和与self-attention相关的layer-norm可视化，它可能长这样。

![img](images/1686799156219-7b91dadc-18d6-487f-8f17-11ca952d66ca.png)

decoder中的子层也一样。考虑transformer中2层叠加的encoder和decoder，它可能长这样：

![img](images/1686799276920-6727213a-64e1-4af8-b2a9-02b1553a18fc.png)

##### 2.10. decoder层

既然已经基本上理解了encoder层，我们也基本上了解了decoder的的组成部分，现在将二者结合起来看。



encoder以处理输入序列开始。encoder顶层输出的是一组attention向量K和V。这些向量被每个decoder用在encoder-decoder attention层，帮助decoder关注输入序列中的正确位置。

![img](images/1686799869232-60e1624e-b922-4c69-928f-4e80df712d5b.gif)



完成encoding阶段后，开始decoding解决。解码阶段的每一步都从输出序列中输出一个单词。

下面的步骤重复这个过程， 直到达到一个特殊标志符，这个标志符表示transformer decoder已经完成它的输出。下一个时间步每一步的输出都喂到底层decoder，与encoder类似，decoders收集他们的decoding结果。就像我们处理encoder的输出一样，我们在decoder的输入中嵌入并加上位置编码以表示每一个单词的位置。

![img](images/1686800396448-d56483a2-cb5b-40bb-ad03-6e7d202e6fa6.gif)



decoder操作中的self-attention与encoder中的self-attention不同。

在decoder中，self-attention只允许关注输出序列中较早的位置。这是通过在self-attention计算的softmax步骤之前设置位置掩码(将他们设置为-inf)来实现的。

除了 encoder-decoder attention从下面的层创建Querie矩阵和从encoder栈的输出中计算Keys和values矩阵之外， encoder-decoder attention与multihead self-attention工作原理一致，

##### 2.11. final Linear层和Softmax层

decoders栈输出浮点型向量，如何将其转化为一个单词呢，这就是softmax层之后的final Layer层做的事。

Linear层是一个简单的全连接网路，这个网络将encoders栈产生的向量投射到一个非常大、非常大的向量中，这个向量叫logits向量。

假设我们的模型知道10000个不同的英文单词(模型的输出字典)， 这些单词都是从训练数据集中学习到的。

这会使得logits向量的宽度达到10000个单元， 每一个单元对应一个单词的分数。这就是我们如何解释线性层之后的输出了。

softmax layer将这个分数转为概率(都是正数， 加起来的和为1）。概率最高的单元被选中，与该单元相关联的单词就作为当前时间步的输出单词。

![img](images/1686801400913-c8d990c0-edbd-41aa-ab10-dd3459a951a3.png)

图片中最下面是decoder栈产生的输出向量。然后将其变成输出单词。

##### 2.12. 训练过程

我们已经了解了训练好的Transformer的整个前向过程。下面直观上看一下模型的训练过程。

在训练过程中，一个没有经过训练的模型会经历完全相同的前向传播过程。因为我们在带有标签的数据集上训练，我们能够将模型的输出与真实的标签进行比较。

为将其可视化，我们假设我们的输出字典只包含6个单词("a", "I", "thanks", "student"，and "<eos>"

![img](images/1686809494962-11687ff7-972a-4328-b875-472a03fe93a4.png)

模型的输出字典在训练阶段之前的预处理阶段创造的。



一旦我们定义了输出字典，我们可以用同样大小的向量表示字典中的字。这就是大家熟知的one-hot编码，例如，我们可以用下面的向量表示单词表示"am”

![img](images/1686809807045-cffa7a9a-5a92-4aa8-aa79-b27e6de6de16.png)

举例：输出字典的one-hot编码



继续讨论模型的损失函数——在训练过程中优化的矩阵以形成一个经过训练、效果惊人的模型。



##### 2.13. 损失函数

假设我们正在训练我们的模型，假设是我们训练的第一步，而且我们在训练一个非常简单的例子——将“merci"翻译成"thanks".

它的意思就是我们想要输出的概率分布指向单词"thanks"， 但是模型并未经过训练，现在还不可能发生。

![img](images/1686810401467-5169afcf-9b0d-4b23-8803-aac5534ef0cc.png)

因为模型的权重是随机初始化的，未训练的模型产生了概率分布，每一个单元格都是随机值。我们能将它与真实的输出进行对比，然后使用反向传播调整模型的权重让输出更加逼近理想的输出。



如何比较两个概率分布？只需从另外一个中减去一个。更多的细节，可以查看交叉熵损失。

但需要注意的是，这是一个过于简单的案例，实际上，我们使用的是更长的句子而不是一个单词。例如——输出：“je suis étudiant”，期望的输出是“I am a student”. 真实的含义是：我们需要我们的模型成功输出这样的概率分布：

- 每一个概率分布都由一个字典大小的向量表示。（我们的例子中是6， 但实际中可能是30000或50000）
- 第一个概率分布在与单词"I”相关的单元格上有最高的概率分布
- 第二个概率分布在与单词"am”相关的单元格上有最高的概率分布
- 以此类推，直到第5个输出分布指向"<end of sentence>"标志，它也有一个与之相关的10,000元素词汇的单元。

![img](images/1686811372003-e4b71494-7824-4e4f-8067-b248ab3c053d.png)

我们将在训练实例中针对一个样本句子的目标概率分布来训练我们的模型。

训练一段时间后，我们期望的输出是这样的：

![img](images/1686811583000-d2a6c63c-ce7d-478e-983e-f5b7d01af03e.png)

训希望在训练时，模型能输出我们期望的正确翻译。当然，这句话是否是训练数据集的一部分并没有真正的迹象（见：交叉验证）。请注意，每个位置都有一点概率，即使它不可能成为该时间步骤的输出 -- 这是softmax的一个非常有用的属性，有助于训练过程。



现在，由于该模型一次产生一个输出，我们可以假设该模型是从该概率分布中选择概率最高的词，而丢弃其他的。这是一种方法（称为贪婪解码）。另一种方法是保留，比如说，前两个词（比如说，'I'和'a'），然后在下一步，运行模型两次：一次假设第一个输出位置是'I'，另一次假设第一个输出位置是'a'，考虑到1号和2号位置，哪个版本产生的误差小就保留。我们对2号和3号位置重复这一步骤......等等。这种方法被称为 "波束搜索"，在我们的例子中，beam_size是两个（意味着在任何时候，两个部分假设（未完成的翻译）都被保留在内存中），top_beams也是两个（意味着我们会返回两个翻译）。这两个都是超参数，你可以进行实验。



#### 3. The Illustrated Bert, ELMo（NLP如何突破迁移学习）

##### 3.1. 简介

2018年是机器学习模型文本处理的拐点(更准确的说： NLP）。我们对以何种方法更好的表达单词和句子之间的潜在意义和关系的概念性理解正在发生变革。而且，NLP社区已经提出了强大的组件，你可自由下载并在模型和基线中使用。(它被称为NLP的ImageNet时刻，它指的是多年前类似的发展加速了机器视觉任务的发展)

![img](images/1686878578510-d0700937-e749-4060-bc25-3c183f378e19.png)

Bert的发布是NLP发展中的一个里程碑，可以说是标志着NLP的一个新纪元。Bert在模型如何更好的处理基于语言的任务中打破了多项记录。描述模型的文章刚发布不久， 团队就开源了模型的源代码，而且允许下载已经在大规模数据集上预训练好的预训练模型。这是一个重要的时刻，因为它让任何人都能像使用准备好的组件那样搭建涉及语言处理相关的机器学习模型——节省从头开始训练语言模型的时间、精力、知识和资源。

![img](images/1686880095892-fe08ebbf-624a-4097-ac2a-831b51673ccc.png)

BERT只需两步，第一步下载预训练模型(在未经注释的数据上进行训练)，第二步考虑如何去微调。

BERT建立在最近在NLP社区涌现出的一些聪明的想法之上——包括但不限于半监督式序列学习， ELMo， ULMFiT和OpenAI transformer以及transformer。

为了正确理解BERT是什么，人们需要了解一些概念。因此，在研究模型本身所涉及的概念之前，让我们先来看看你可以使用BERT的方法。

##### 3.2. 例子：句子分类

使用BERT最直接的方法是用它来对单一的文本进行分类。这个模型看起来像这样：

![img](images/1686881506697-55a699c5-5cd1-44e1-b300-26f6a97c26a3.png)

为训练这个模型，你主要需要训练一个分类器，在训练阶段对BERT模型进行最下的改变。这个训练过程叫作微调， 其根源在半监督学习和ULMFiT。



对那些不熟悉该话题的人来说，我们在谈论的是分类器，然后我们是在机器学习的监督学习领域。这意味着我们需要一个标注好的数据去训练这个模型。对于垃圾邮件分类的例子，标注的数据是邮件信息的列表和一个标签。

![img](images/1686882185244-092cfb19-2792-4dc7-9dff-79946b297660.png)

使用这种情况的其他例子包括：

- 情感分析

- - 输入：电影/商品评论。输出：评论正面 or 负面
  - 案例数据：https://nlp.stanford.edu/sentiment/

- 事实核查

- - 输入：句子，输出：索赔 or 不索赔

##### 3.3. 模型结构

通过案例已经知道了BERT模型如何使用，现在来看一下模型是如何工作的。

![img](images/1686882519740-4204600e-e8a4-4e7b-9837-41e61586cc10.png)

论文介绍了BERT模型的两种模型规模：

- BERT BASE——为了比较性能，与OpenAI Transformer规模相当
- BERT LARGE——一个大模型，达到了论文中的最优水平。

BERT本质上式一个训练好的Transformer Encoder堆叠结构。

![img](images/1686883016504-66c75956-6f71-495e-9690-3355c702bdc8.png)



BERT模型有多个encoder层(论文中称为Transformer Blocks)——Base version中有12个，Large version中有24个。还有非常大的前馈网络层(分别有768和1024g个隐藏单元)、以及分别有12和16个多头注意力。而原始论文中的默认配置中只有6个encoder层，512个隐藏单元和8个注意力头。

##### 3.4. 模型输入

![img](images/1686883382318-97ed19ec-c988-4a78-a3e8-24527ff5e3f6.png)

第一个输入标记有一个特别的[CLS]标记，其原因在后文中阐述。这里CLS代表分类。

就像transformer中的encoder一样，BERT的输入也是一系列字， 这些字在堆栈中流动。每一层使用self-attention， 然后将其结果发送到前馈神经网络层，然后将其发送给下一个encoder。

![img](images/1686883653035-e59c50b1-a695-4e55-b298-b10d14f345f9.png)

在模型结构而言，到目前为止，这与Transformer完全相同(除了尺寸之外，尺寸我们可以在配置中设置)，在输出端，我们第一次看到了不同之处。

##### 3.5. 模型输出

每个位置都输出隐藏层大小的向量(在BERT Base中是768)。针对上面的分类案例，我们关注输出的第一个位置(这个位置我们输入的是[CLS]标记。

![img](images/1686883982303-24ef7bdd-acb8-40b5-843c-02ef33c9d4d7.png)

这个向量可以作为我们选择的分类器的输入。论文中使用一层神经网络作为分类器就取得了非常好的结果。

![img](images/1686884148797-bb57f222-7043-4efd-9bcf-e24ea0de8659.png)

如果你有更多的标签(比如：如果你的邮件服务中的标签为：spam 、 not spam  、social 、promotion)，你只需要调整分类器网络将更多的输出神经元，然后经过softmax处理。

##### 3.6. Parallels with Convolutional Nets

对于有机器学习背景的人来说，这种向量处理应该联想到像VGGNet和末端全连接分类网络。

![img](images/1686884575090-a17be9ee-6971-401a-8cd3-8da213eec7d8.png)

##### 3.7. embedding新时代

新的发展带来了单词嵌入方式的新转变。到目前为止， 词嵌入是NLP模型处理语言的最主要推动力。像Word2vec、Glove这样的方法被广泛使用。现在回顾一下是如何使用的。

###### 3.7.1. word embedding 回顾

对于被机器学习模型处理的单词而言，他们需要形成模型可用于计算的数值表征。Word2vec表明我们可以使用向量表示单词，这种向量可以捕捉语义信息、相似或相对单词的意义或者像这种有相同关系的单词对。

该领域很快意识到，使用在大量文本数据上预训练的嵌入是个好主意，而不是在经常是小数据集上与模型一起训练它们。因此，有可能下载一个由Word2Vec或GloVe预训练产生的单词和它们的嵌入列表。这是一个GloVe嵌入 "stick "这个词的例子（嵌入向量大小为200）。

![img](images/1686885205553-3a55fbf2-2f27-4ebd-b8d9-7328d31c0808.png)

GloVe对 "stick "这个词的嵌入--一个由200个浮点数（四舍五入到两位小数）组成的向量。一直到两百个数值。

由于这些数字很大，而且充满了数字，所以我在文章中的数字中使用以下基本形状来显示向量：

![img](images/1686885255576-8ec0800d-19dd-4e48-92d4-ad3649e4bf60.png)

##### 3.8. ELMO: context matters【语境很重要】

如果我们使用这个GloVe表示法，那么不管上下文是什么，"stick "这个词都会由这个向量来表示。"wait a minte"，一些NLP研究人员说，"stick""有多种含义，取决于它的使用场合。为什么不根据它的使用环境给它一个嵌入--既能捕捉到该语境中的词义，又能捕捉到其他语境信息？"。于是，语境化的词嵌入诞生了。

![img](images/1686886231763-e7b46b15-d427-4f42-acb1-807508777251.png)

ELMo不是为每个词使用一个固定的嵌入，而是在给其中的每个词分配一个嵌入之前查看整个句子。它使用一个在特定任务上训练的双向LSTM，以便能够创建这些嵌入。

![img](images/1686886298654-c20624a1-2087-49fd-ac31-fa25443ce810.png)

ELMo为NLP背景下的预训练提供了重要的一步。ELMo LSTM将在一个大规模的数据集上以我们的数据集的语言进行训练，然后我们可以把它作为其他需要处理语言的模型中的一个组件。

ELMo的秘密是什么？

ELMo从被训练来预测一连串单词中的下一个单词中获得了它的语言理解--这项任务被称为语言建模。这很方便，因为我们有大量的文本数据，这样的模型可以从中学习而不需要标签。

![img](images/1686886425144-d91c7655-699e-4f6e-a2e2-3ec1ea85803c.png)

ELMo预训练过程中的一个步骤：给定 "Let's stick to "作为输入，预测下一个最有可能的词--这是一个语言建模任务。当在一个大的数据集上进行训练时，模型就会开始捕捉到语言模式。在这个例子中，它不太可能准确地猜出下一个词。更现实的是，在 "hang "这样的词之后，它将为 "out"（拼写 "hang out"）这样的词分配一个比 "camera "更高的概率。

我们可以看到每个unrolled-LSTM的隐藏状态从ELMo's的头部后面冒出来了，在这个预训练完成后，这些在嵌入过程中会很方便。

ELMo通常会更深入一些并且训练一个双向LSTM——因此它的语言模型并不会只对下一个单词有感知，对前面的单词也有感知。

![img](images/1686898054740-bcbe2b59-a0b8-4aaa-8fca-c3b201e1934a.png)

ELMo通过将隐藏状态(和初始嵌入)以某种方式分组(连接后加权求和)得到了上下文嵌入。

![img](images/1686898305518-abb6a4ad-8ed8-4b3e-9166-1dd4102b7e39.png)

##### 3.9. ULM-FiT: NLP中的迁移学习

ULM-FiT引入了在模型预训练阶段学习到的大量高效有用的方法——不仅仅是词嵌入和上下文词嵌入。ULM-FiT提出了语言模型和对不同任务进行有效微调的过程。

NLP最终可以像计算器视觉一样进行模型微调。

##### 3.10. Transformer, 超越LSTM

Transformer论文和源码的发布，以及在机器翻译等任务上达到的效果开始让某些人认为Transformer可以取代LSTMs了。此外，Transformers比LSTMs更好的处理了长期依赖关系， 这使得情况更为复杂。

transformer中的Encoder-Decoder结构让它更适合做机器翻译任务。但如何让它做句子分类呢？如何让它微调后能适应其他任务呢？

##### 3.11. OpenAI Transformer: Pre-training a Transformer Decoder for Language Modeling

事实上我们并不需要transformer去适应迁移学习和一个NLP任务的微调的语言模型。我们仅仅只需要处理transformer中的decoder。decoder是一个很好地选择因为它是语言模型(预测下一个单词)的自然选择， 因为它是通过mask下一个tokens而建立的——它是一个在生成逐字翻译中的有价值的功能。

![img](images/1686899599245-93e535b3-70d6-41cd-ba68-1fbdc8aa4b89.png)

OpenAI中的transformer是由transformer中的decoder堆叠而成。

模型堆叠了12层decoder。在设置中没有encoder，这些decoder层不会有encoder-decoder注意力子层， 而原始的transformer decoder层有。它仍然有attention层（经过屏蔽，所以它不会在未来的标记上达到峰值）

有了这样的结构，我们可以开始在相同的语言模型任务上去训练这个模型：使用大量数据集去预测下一个单词。仅仅丢给它7000本书让它学习。对于这些任务来说，书实在太多了。因为它会让模型去学习相关联的信息即使它们被大量的文字隔开——例如，当你用推文或文章进行训练时，你不会得到这样的东西。

![img](images/1686900054304-75f58184-660c-4e77-856c-efacec371f30.png)

现在，OpenAI transformer已经准备好接受训练，在由7000本书组成的数据集上预测下一个单词。

##### 3.12. 下游任务的迁移学习（Transfer Learning to Downstream Tasks)

既然openAI transformer是被预训练的，模型的层已被调整到可以合理的处理语言，我们可以从下游任务中开始去使用它。首先来看一下句子分类。

![img](images/1686900350392-716c6040-58fc-49fa-aa4e-270d0bf70a87.png)

如何使用预训练的OpenAI transformer模型进行句子分类

OpenAI的论文概述了一系列的输入转换，以处理不同类型任务的输入。论文中的以下图片显示了模型和输入转换的结构，以执行不同的任务。

![img](images/1686901601402-ec5cdf02-dce5-43bd-bdd3-a9387b7fa3f3.png)

##### 3.13. BERT:　从Decoders到Encoders

openAI Transformer提供了基于Transformer的微调的预训练模型。但从LSTMs到Transformers的转变中丢失了一些东西。ELMo的语言模型是双向的，但是transformer只训练了前向语言模型。我们能否训练一个基于transformer模型的前向和后向模型呢？

###### 3.13.1. 掩码语言模型

![img](images/1686901977973-dd4a6bad-d375-481f-93fc-dadec7f08002.png)

BERT的语言模型的输入中有15%的掩码，模型会去预测这些丢失的单词

找到正确的任务来训练编码器的Transformer堆栈是一个复杂的障碍，BERT通过采用早期文献中的 "遮蔽语言模型 "概念（在那里它被称为Cloze任务）来解决这个问题。

除了屏蔽15%的输入外，BERT还将事情混在一起，以改善模型后来的微调。有时，它随机地将一个词替换成另一个词，并要求模型预测该位置的正确单词。

###### 3.13.2. Two-sentence 任务

如果你回头看看OpenAI Transformer为处理不同的任务所做的输入转化，你会注意到有些任务要求模型对两个句子说一些智能的东西（例如，它们是否只是彼此的转述版本？给出一个维基百科条目作为输入，而关于该条目的一个问题作为另一个输入，我们能回答这个问题吗）。

为了使BERT更好地处理多个句子之间的关系，预训练过程包括一个额外的任务： 给定两个句子（A和B），B是否可能是A后面的句子，或者不是？

![img](images/1686902418030-bf76f429-35cc-4c10-b2aa-879ca6df973f.png)

BERT预训练的第二个任务是一个两句话的分类任务。在这个图形中，标记化被过度简化了，因为BERT实际上使用WordPieces作为标记而不是单词------所以一些单词被分解成更小的块。

###### 3.13.3. Task specific-Models

BERT论文展示了一些将BERT用于不同任务的方法。

![img](images/1686902637127-069910b1-faaa-4177-9a2d-06f431894ebe.png)

###### 3.13.4. BERT for feature extraction

微调的方法并不是使用BERT的唯一方法。就像ELMo一样，你可以使用预先训练好的BERT来创建语境化的单词嵌入。然后，你可以将这些嵌入送入你现有的模型--该论文显示，在诸如命名实体识别这样的任务上，这一过程产生的结果与微调BERT相差无几。

![img](images/1686902748407-58e55447-c6ec-426d-822e-80269ab2a58f.png)

哪种矢量作为语境化嵌入的效果最好？我认为这取决于任务。本文研究了六种选择（与取得96.4分的微调模型相比）：

![img](images/1686902820713-154d7d03-9020-4d55-b7b0-33f62ccded49.png)

##### 3.14. Take BERT out for a spin





#### 4. GPT-2可视化

今年，我们看到了很多机器学习应用。GPT-2展现了令人印象深刻的能力，它能写出连贯且富有激情的文章，超过了我们预期的当前语言模型所能产生的效果。GPT-2并不是一个特别新颖的架构——它的架构与纯decoder的transformer非常相似，然而，GPT-2是一个非常大的，基于transformer的语言模型，并在一个大规模的数据集上进行训练。在这篇文章中，我们将研究使该模型产生结果的架构。我们将深入理解它的self-attention层并看看其他除语言模型之外的纯decoder transformer应用。

这篇文章是对“The illustrated transformer"的补充，更细致化的解释transformer的内部机制以及自原始论文以来的演变过程。我们希望可视化语言可以更容易理解基于transformer的模型， 因为它们内部的原理在不断变化。

##### 4.1. GPT2和语言模型

什么是语言模型。

In The Illustrated Word2vec这篇文章中， 我们会看到什么是语言模型——它是一个机器学习模型，能够根据句子的一部分去预测下一个单词。最著名的语言模型是智能键盘，该键盘能基于当前的输入预测下一个单词。

在某种意义上，GPT2基本上就是键盘应用下一个单词预测功能。但它更大更复杂。GPT-2是在名为WebText的巨大的40G数据集上训练的。OpenAI的研究人员工互联网上抓取一部分数据作为研究。为了比较存储大小，我使用的键盘应用程序SwiftKey占用了78MB的空间。训练有素的GPT-2的最小变体，需要占用500MB的存储空间来存储其所有参数。最大的GPT-2变体是13倍的大小，所以它可能占用超过6.5GB的存储空间。

![img](images/1687141571410-a1242f92-3cfd-420e-af69-ea790a5dd68b.png)

探索GPT-2的一个好方法是使用AllenAI GPT-2 Explorer。它使用GPT-2来显示下一个单词的10个可能预测(以及它们的概率得分). 你可以选择一个单词，然后看到下一个预测列表，继续这段对话。

##### 4.2. 语言建模Transformers

像我们在The Illustrated Transformer中看到的一样，原始的transformer是由encoder和decoder堆叠而成， 我们可以称为transformer blocks。这种架构是合适的，因为该模型解决了机器翻译问题--这是一个encoder-decoder架构在过去取得成功的问题。

![img](images/1687142333338-2b31b46c-8677-41e7-bb9c-62a6916b1dd3.png)

在随后的许多研究工作中，该架构舍弃了encoder-decoder。 只使用一个堆叠的transformer模块——将它们堆叠得更可能高， 喂给它们足够多的训练文本以及大量的计算。（训练其中一些语言模型需要数十万美元，在AlphaStar的情况下可能需要数百万美元）

![img](images/1687142806219-cfa0afa6-7db2-4edf-9bf8-c4fa59cb5284.png)

我们如何堆叠这些block，事实证明，这是不同GPT2模型尺寸之间的主要区别因素之一。

![img](images/1687142916396-9687c8bd-4679-4719-a1da-12ec9a36f8ca.png)

**与BERT的一个不同点**

**GP**T2使用transformer decoder模块建立的，BERT使用transformer encoder模块。在下面的章节中继续探索它们的不同。我们将在下面一节中研究两者的区别。但两者之间的一个关键区别是，GPT2，像传统的语言模型一样，一次输出一个标记。例如，让我们提示一个训练有素的GPT-2来背诵机器人学的第一定律：

![img](images/1687143361432-9b1862d6-e9b2-4128-b19b-4aad76f1621a.gif)

这些模型的实际工作方式是，在每个标记产生后，该标记被添加到输入序列中。而这个新的序列成为模型下一步的输入。这是一个叫做 "自动回归 "的想法。这是使RNN变得不合理地有效的想法之一。

![img](images/1687143483642-5143388c-6670-4cb0-a2f2-b458adc6b181.gif)
GPT-2, 以及像后来出现的TransformerXL和XLNet本质上都是自回归模型。Bert不是。这是一个权衡。在失去自动回归的情况下，BERT获得了将一个词的两边的上下文结合起来的能力，以获得更好的结果。XLNet带回了自回归，同时找到了另一种方式结合上下文的方法。

##### 4.3.  Transformer模块的变革

transformer最开始的论文[https://arxiv.org/abs/1706.03762]中包含两种transformer blocks:

###### 4.3.1. The Encoder Block

第一个是encoder block

![img](images/1687144858229-95f2bed2-f854-4d52-bd09-c9af867f06eb.png)

原文中的encoder模块可以接受输入，直到某个最大序列长度（例如512个符号）。如果一个输入序列比这个限制短，也没关系，我们可以直接把序列的其余部分垫起来。

###### 4.3.2. The Decoder Block

其次，是decoder模块，它与encoder模块在结构上有一个小的变化--有一层允许它关注来自encoder的特定片段：

![img](images/1687145018846-bafe320e-9f79-4027-b255-a09219e3aa05.png)

self-attention层的一个关键不同是它掩盖了未来的token——不像BERT那样仅仅将单词变为[mask]，而是通过干扰自我注意的计算，阻止来自正在计算的位置右边的标记的信息。

例如，如果我们要强调4号位置的路径，我们可以看到，它只允许参加现在和以前的标记：

![img](images/1687145358447-28420697-1e81-484f-b7e8-1592c78089c3.png)

重要的是，self-attention（BERT使用的）和被掩盖的self-attention（GPT-2使用的）之间的区别是明确的。普通的self-attention允许一个位置在其右边的令牌上达到峰值。掩码self-attention可以防止这种情况发生：

![img](images/1687145550488-07a8ae67-4930-4861-9827-7fe0846bdc6b.png)

###### 4.3.3. The Decoder-Only Block

在原始论文之后， [Generating Wikipedia by Summarizing Long Sequences](https://arxiv.org/pdf/1801.10198.pdf) 提出了另外一个transformer的安排， 它非常适合做语言建模。该模型舍弃了Transformer encoder. 对于这个原因，我们称这个模型为“Transformer-Decoder". 早期基于transformer的语言模型是由6个transformer decoder模块堆叠而成。

![img](images/1687145901704-aacafc1f-e264-4767-bcc5-de769c39b4bc.png)

解码器模块是相同的。我扩大了第一个，所以你可以看到它的self-attention层是被掩码的变体。请注意，该模型现在可以在某一段落中处理多达4000个tokens--与原始transformer中的512个相比，这是一个巨大的升级。

这些模块与原始的decoder模块非常相似，只是它们取消了第二层的self-attention层。在《具有更深自我关注的字符级语言建模》【https://arxiv.org/pdf/1808.04444.pdf】中研究了一个类似的架构，以创建一个语言模型，每次预测一个字母/字符。

OpenAI GPT-2使用单一decoder模块。

##### 4.4. Looking inside GPT-2

把一个训练好的GPT-2放在手术台上看看它是如何工作的。

![img](images/1687146473945-21a7b2b6-7a3e-42f9-815c-d80707c6dbe2.png)

GPT-2能处理1024个tokens,每个tokens沿着它自己的路径流经所有的decoder模块

运行训练好的GPT-2的最简单方法是让它自己漫游(技术上称为生成无条件样本)——或者我们给它一个提示，让它讲一个特定的主题(生成交互式条件样本)。 在漫游案例中，我们能简单的把开始标记交给他， 让它开始生成单词(训练的模型使用<|endoftext|>作为开始标记，现称之为<s>)

![img](images/1687147099307-eae6b2c2-e247-4d3e-95e4-01a3aea6d077.gif)

模型只有一个输入token，所以只有一个激活路径。token在所有层中被连续处理，然后沿该路径产生一个向量。

该向量可以根据模型的词汇量进行评分，(模型知道的所有单词，在GPT-2的情况下为50,000个单词)。在这种情况下我们选择得到最高的token，"the"。但是，我们当然也可以把事情搞混——你知道如果你在键盘应用中不断点击建议的单词，它有时会陷入重复的循环中，唯一的出路就是你点击第二个或第三个建议的单词。这种情况同业也会发生在模型中，GPT-2有一个叫做top-k的参数，我们可以用它来让模型考虑对顶层词以外的词进行采样（top-k=1时就是这种情况）。

在下一步，我们将第一步中的输出添加到输入序列中，然后让模型做下一步预测：

![img](images/1687155308097-814993ab-98e6-45b3-bf7e-96eedd24c379.gif)

注意到再这次计算中只激活了第二条路径。GPT-2的每一层都保留了它自己对第一个token的解释，并将在处理第二个token时使用它(接下来的部分会更细致的讲解self-attention).GPT-2并没有根据第二个token来重新解释第一个token。

###### 4.4.1. 输入 Encoding

让我们看看更多的细节，以便更深入地了解这个模型， 就像我们之前讨论的NLP模型中一样，模型会在embedding matrix中寻找输入单词的embedding——这是我们训练模型得到的部分组件。

![img](images/1687155931254-6688fcef-f998-4e8f-be31-38e474fe5d27.png)

每一行都是一个词的嵌入：一个代表一个词的数字列表，并捕捉到它的一些含义。该列表的大小在不同的GPT2模型尺寸中是不同的。最小的模型使用每个词/令牌的嵌入大小为768。

所以在开始，　我们会在embedding matrix中查找开始标志<s>的嵌入。在将其移交给模型中的第一个块之前， 我们需要结合位置encoding——它表示输入序列中单词顺序的信号。训练好的模型的一部分是一个矩阵，它包含了输入中1024个位置的位置编码向量。

![img](images/1687156390888-b405d1da-329b-492e-9ae2-ca260144fdc9.png)

有了这些，我们已经知道了输入词在被交给第一个转换块之前是如何处理的。我们还知道了构成训练后的GPT-2的两个权重矩阵。

![img](images/1687158003753-0d76dd70-6ea5-4445-ad0a-7780b97fa8fb.png)



###### 4.4.2. A journey up the stack

第一个区块现在可以处理token，首先通过self-attention过程，然后通过其神经网络层。一旦第一个transformer 处理了这个token，它就将其产生的向量送上堆栈，由下一个块来处理。在每个blocks中的过程是相同的，但是每一个block在self-attention层和神经网络层都有它自己的权重 。

![img](images/1687156954387-d4218bd3-a5f9-47bf-8995-12af2c3a6fa9.png)

###### 4.4.3. self-attention回顾

语言严重依赖上下文，举个例子

“A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.

下面有三处需要依赖其他单词的单词，并将其做高亮显示。如果不结合上下文背景就无法理解和处理这个单词。当模型处理这个句子时，必须要知道：

1. it refers to the robot
2. such orders refers to the earlier part of the law, namely "the orders given it by human beings"
3. The First Law refers to the entire First Law

这就是self-attention的作用。它在处理某个词（通过神经网络）之前，将模型对相关和关联词的理解融入其中，以解释该词的背景。它的做法是给语段中的每个词的相关程度打分，并将它们的向量表示相加。

举个例子，上面的这块self-attention层处理"it"这个单词时会关注"a robot”， 传递给神经网络的向量是每个向量与分数的乘积再求和。

![img](images/1687157843495-33c4dea7-88e5-407c-b6a2-2ab98f07ee6c.png)

###### 4.4.4. self-attention

自我注意是沿着片段中每个标记的路径处理的。重要成分是三个向量：

- Query:　query是当前单词的表示，用于与所有其他单词（使用它们的键）进行评分。我们只关心我们目前正在处理的标记的查询。
- Key: key向量就像片段中所有单词的标签。它们是我们在搜索相关词语时的匹配对象。
- Value: value是真实的单词表征，一旦我们对每个相关单词进行打分，这些值加起来就表示当前单词。

![img](images/1687158484242-99485c41-7fc1-4290-9213-2b9e4e8e15f5.png)

一个粗略的比喻是把它想成是在文件柜中搜索。query就像是正在研究的主题的便利贴。key就像是文件柜中文件夹的标签。当你将标签与便利贴匹配时，我们会取出文件夹中的内容，这些内容就是value向量。只是你不只在寻找一种value，而是从混合文件夹中寻找一种混合的value。

将每个key向量与query向量相乘得到每个文件夹的分数(技术上：是经过softmax后再点乘)

![img](images/1687158950864-a29534f5-5c70-4b75-8a19-08bb7afabfa4.png)

我们将每个value与它的分数相乘再求和——得到了我们的self-attention输出结果。

![img](images/1687159267257-1331fd31-8131-4801-bd1c-f7a2f96f9482.png)

这种加权混合的向量的结果是：该向量在"robot"上付了50%的"attention"， 在"a"单词上付了30%, 在"it"单词上付了19%。 文章的后面，我们会更深入的探索self-attention。但首先，让我们继续我们的旅程，朝着模型的输出方向前进。

###### 4.4.5. Model Output

当上面的block产生了它的输出向量(self-attention的结果，然后是它的神经网络)时， 模型将向量乘以嵌入矩阵。

![img](images/1687159738817-5929d718-8bf5-4686-a2f2-1cf834d38c82.png)

回顾一下，嵌入矩阵的每一行都对应于模型词汇中的一个词的嵌入。乘法的结果被解释为模型词汇中每个词的得分。

![img](images/1687159956145-238e9fa0-153b-4831-b524-7c226921dcd5.png)

我们简单的选择得分最高(top_k=1)的token，但如果模型考虑其他的单词也可得到更好的结果。所以更好的策略是从整个列表中抽出一个单词， 使用分数作为选择该词的概率(所以得分更高的单词更有可能被选中)。一个折中方案是设置top_k为40， 并让模型考虑得分最高的40个单词。

![img](images/1687161239582-00aad5e9-f330-45c1-9639-97ab79915a65.png)

就这样，模型完成一个迭代，输出一个单词。该模型继续迭代，直到产生整个上下文（1024个标记）或产生一个序列结束的标记为止。

##### 4.5. End of part #1：The GPT-2

现在我们已经知道了GPT-2是如何工作的，如果你对self-attention层的内部很好奇，接下来给你的是奖励部分。我创建它是为了引入更多的可视化语言来描述自我注意，以使描述后来的transformer模型更容易检查和描述（看着你，TransformerXL和XLN）

我想指出这篇文章中的一些过于简单的说法：

- 我交替的使用"words"和"tokens"， 但实际上，GPT2使用字节对编码在字典中创建tokens, 这意味着tokens通常是一对单词。
- 我们展示的例子在推理评估模式爱运行GPT-2。这就是为什么它每次只处理一个单词。训练时，模型针对较长的文本序列进行训练并且一次会处理多个tokens。同时在训练时，另外，在训练时，该模型将处理更大的批处理量（512），而评估时使用的批处理量为1。
- 为了更好的管理图像空间，在旋转和移动向量上我采取了更自由的做法。实现时，这个过程必须更加精准。
- Transformers使用更多层的normalization, 这个过程非常重要。我们在《Illustrate transformers》中注意到了其中的一些内容，但在这篇文章中更多地关注了self-attention。
- 有的时候，我需要显示更多的方框来表示一个矢量。我把这些表示为 "zoom in"。比如说：

![img](images/1687162463533-1d52adb1-10be-46aa-8033-919656ab9f51.png)

##### 4.6. The Illustrated self-attention

文章的早些时候，我们展示了这张图片，用来表示self-attention被用在处理单词"it"的层中。

![img](images/1687162664438-4f1d5f06-7979-49a7-b4cf-1e0b528d89ac.png)

在这部分，我们会深入探讨它是如何工作的。请注意，我们将以一种方式来看待它，试图使个别词发生的情况变得合理。这就是为什么我们会展示很多单一的向量。实际的实现是通过将巨大的矩阵相乘来完成的。但我想把重点放在这里发生在单词层面的直觉上。

###### 4.6.1. self-attention(without masking)

先来看看encoder模块中最原始的self-attention。现在来看一下一次只能处理4个tokens的transformer模块。

self-attention主要通过3个步骤进行的。

- 创建每条路径上的Query，Key, Value向量
- 对每个输入token，使用它的query向量为其他所有的key向量打分。
- value向量与它们对应的分数相乘后再求和。

![img](images/1687163284723-724e0643-17d7-4fa2-bd9f-7e6655b06a0d.png)

###### 4.6.2. 创建Quey,key和value向量

现在关注第一条路径，我们会将query与其他所有的keys进行比较。这会为每个key打分。self-attention的第一步是为每个token路径计算三个向量(现在不考虑attention head)

![img](images/1687163481114-645d196a-ddf0-43c1-97b3-27ffb78aefbe.png)

###### 4.6.3. Socre  打分

既然我们已经有了向量，我们只在step2中使用query和key向量。因为我们已经关注第一个token，我们将所有其他的key向量与query相乘得到每4个tokens中每个token的得分。

###### 4.6.4. Sum 求和

我们将这些分数与value向量相乘，我们将它们求和之后，有更高分数的value会构成所得向量的很大一部分。

![img](images/1687164086673-5b61f4a3-b262-47ff-9796-3926f5849212.png)

分数越低，我们展示的value向量就更透明。这是为了表明乘以一个小数字是如何稀释向量的值的。

如果我们为每一条路径都做相同的操作，我们会得到代表每个token的向量，包含该标记的合适的上下文。然后将这些信息提交给transformer模块中的下一个子层（前馈神经网络）：

![img](images/1687164417583-9fa59a4c-3627-4a20-90a9-528f158695f4.png)

###### 4.6.5. 图解掩码self-attention 【The Illustrated Masked Self-Attention】

既然我们已经深入探讨了transformer中self-attention步骤，让我们继续看一下masked self-attention。除了step2外，掩码self-attention与self-attention相同。假设模型两个tokens作为输入，并且我们正在观察第二个token。在这种情况下，最后的两个token被掩盖了。所以这个模型会干扰 打分的步骤。它总是对未来的token打分为0，所以模型无法在未来单词上取得峰值。

![img](images/1687164908052-9a9b45f9-4a62-465a-ba25-2bc2216f96e5.png)

这个掩码通常以矩阵的形式展示，称为注意力掩码。考虑四个单词组成的序列(例如：“robot must obey orders"）。 在语言建模场景，这个序列可以概括为4个步骤——一个单词一个步骤（暂时假设每个单词时一个token)。因为模型分批次进行工作，我们假设批次为4， 它将作为一个批次处理整个序列。

![img](images/1687165344808-c7c4604e-1f4a-437d-b58f-a1eb6f284379.png)

在矩阵形式中，我们将query矩阵与key矩阵相乘计算一个得分。我们把它想象成以下样子：只是在该单元格中的不是单词，而是与该单词相关的query(或key)向量。

![img](images/1687165594986-c60f37d0-b812-4a6a-b5f4-52ee9d5303df.png)

相乘之后，我们戴上注意力掩码三角形。将我们的掩码部分的单元格设置为-inf或者一个非常大的负数(在GPT2中设置为-1千万).

![img](images/1687166639084-ccf38871-d428-44e8-b271-ef8df10941f5.png)

然后对每一行进行softmax操作，得到我们用于self-attention的真实分数。

![img](images/1687166739025-68c00858-dd97-4544-ae4b-5d4c395e0681.png)

分数表的含义如下：

- 当模型处理数据集中的第一个例子时(row #1)， 该行只包含一个单词"robot"， 因此会给与该单词100%的关注。
- 当模型梳理数据集中的第二个例子时(row #2)， 该行包含“robot must"， 当处理单词"must"时， 在robot上会给与48%的关注，52%的关注分配给单词"must"
- 以此类推

###### 4.6.6. GPT-2掩码self-attention

继续探索GPT-2的掩码attention

评估时间：一次处理一个Token

我们可以使GPT-2的运作与掩盖的自我注意的运作完全一样。但在评估阶段，当完成一个迭代后我们的模型只添加一个新的单词， 沿着早期路径重新计算已经处理过的token的self-attention并不高效。

在这种情况下，我们处理第一个token(现在不考虑<s>）

![img](images/1687167371527-09a6298e-0f9c-4652-a6e4-296f776ea66a.png)

GPT-2带有"a"这个token的key和value向量 。每一个self-attention层都带有那个token的key和value向量。

![img](images/1687167566734-21a9a72b-ca33-4155-a4a6-98587716adf3.png)

在下一个迭代中，但模型处理单词"robot"时， 它并不需要产生token "a"的query、key和value， 它只是重复使用第一个迭代过程中保存下来的query、key和value

![img](images/1687167731828-d556dd2b-8468-466f-9ca0-88d59fc3fcf4.png)

**GPT-2self-attention: 1-创建queries、keys和values**

我们假设模型正在处理单词"it"， 如果我们正在讨论底层的block， 然后该token的输入可能是单词it的embedding+位置#9的位置编码。

![img](images/1687168059107-d9825cc7-765a-45a4-aa23-331532e956f7.png)

transformer中的每一个block都有它自己的权重, 我们面临的第一个权重矩阵就是我们用来创建queries、keys和values的权重矩阵。

![img](images/1687168223911-9bb063e3-f683-44fc-8ecc-2cc48499d018.png)

self-attention将输入与权重矩阵相乘（还需要加上一个偏置矩向量，图中没有展示)

相乘会得到一个结果向量，这个向量基本上是单词"it"的query、key和value向量的拼接。

![img](images/1687168393350-e326f1d4-6760-4b90-851d-1390b80d6506.png)

输入向量与权重向量相乘(再加上偏置)得到该token的key、value、query向量

**GPT-2 self-attention， 1.5 切分attention heads**

在前面的例子中，我们直接进入了self-attention而忽略了"multi-head"部分，现在关注一下这个概念很有帮助。self-attention在K、Q、V的不同部分多次进行运用。切分注意力头是简单的将一个长的向量转变成一个矩阵。微型GPT2有12个注意力头，因此这可能就是变换矩阵的第一个维度。

![img](images/1687314066281-3ebb1e93-92a8-4148-a41f-9b6f67750fb9.png)

在前面的例子中，我们知道了一个注意力头内部的运行机制。想到多个注意力头的一种方法是这样的(如果我们只把12个注意力头中的三个可视化)

![img](images/1687314271214-a2dff9d7-f1e2-464c-bb7b-7ea7733c2488.png)

**GPT-2 Self-attention: 2-打分**

现在可以去打分——我们现在可以进行评分了--知道我们只看了一个注意头（而其他所有的注意头都在进行类似的操作）：

![img](images/1687314515859-24b22444-e3b7-4afc-a8a2-5f21e72476ef.png)

现在，该token可以对照其他token的所有key进行评分（在以前的迭代中，在注意力头1号中计算）：

![img](images/1687314607049-53d4e608-c1d7-4767-8a55-43c775cf5cb6.png)

**GPT-2 Self-attention: 3-Sum**

就像我们之前看到的那样，我们现在将每个value与score得分相乘，然后求和，就得到了attention-head #1的self-attention结果。

![img](images/1687314976924-3edb6a23-2eda-45f9-adfa-653c8ff39766.png)

**GPT-2 Self-attention: 3.5-Merge attention heads**

**我们处理不同注意力头的方法是首先将它们拼接成一个向量。**

![img](images/1687316473838-53bbdae9-4a8e-48e5-a002-e06d5f43f167.png)

但是这些向量不会送到下一个子层中，我们需要将Frankerstein's-monster的隐藏状态转变成同质化表征。

GPT-2 Self-attention: 4-Projecting

我们会让模型去学习如何更好的将self-attention拼接成一个向量，这个向量能被前馈神经网络处理。这里就得到了第二大的权重矩阵，这个权重矩阵将attention heads的结果投射到self-attention子层的输出向量中。

![img](images/1687317201480-786f43ec-0b98-4eb5-b5be-e70966475bb8.png)

有了它，我们可以产生送入下一层的向量。

![img](images/1687317303690-2dd54e3b-31a5-41d1-b29b-c85da1bf18ae.png)

###### 4.6.7. GPT-2 Fully-Connected Neural Network: Layer #1 全连接层

全连接层是在自我注意将适当的背景纳入其表征之后，区块处理其输入标记的地方。它包含两层。第一层是模型大小的四倍(因为GPT2 small是768维， 而网络有768*4=3072个单元)。 为什么是4倍，因为这是原始transformer滚动的地方(模型的尺寸是512维，模型中layer#1的维度是2048). 这似乎可以让transformer模型足够的表征空间去处理扔给他们的任务。

![img](https://cdn.nlark.com/yuque/0/2023/gif/32553153/1687327701372-2354d38c-9ec1-4ff8-822e-f85c6c98d5d5.gif)

**GPT-2 全连接神经网络：第二层，投射到模型维度**

第二层将第一层的结果投射到模型维度(GPT2small是768). 这个乘法的结果是transformer模块的结果。

![img](https://cdn.nlark.com/yuque/0/2023/gif/32553153/1687327720182-984dd751-36ce-43be-950c-11b70235a98c.gif)

这是transformer模块最详细的版本. 你现在已经基本掌握了transformer语言模型内部所发生的绝大多数情况。概括地说，我们的输入向量遇到了这些权重矩阵：

![img](images/1687327928427-94c74453-f336-41f7-afe9-2683c9554faa.png)

每一个模块有三组权重，另外一方便，这个模型只有一个token embedding矩阵和一个位置编码矩阵。

![img](images/1687328022051-fd51553f-be37-4940-999e-c2d68108afc1.png)

如果你想看到模型的所有参数，那么我在这里对它们进行了统计：

![img](images/1687328109156-ff008a87-f95f-4544-9c63-0359b007a098.png)

由于某种原因，他们加起来有124M的参数而不是117M的参数。我不确定为什么， 但在公布的代码中似乎有很多是这样的（如果我错了请纠正我）。

##### 4.7. 第三部分，Beyond Language Modeling

纯decoder的transformer显示了超越语言模型的前景，有很多的应用显示了它的成功，可用上述类似的视觉效果来描述。让我们看一下其中的一些应用来结束这篇文章吧。

###### 4.7.1. 机器翻译

一个encoder无法完成翻译，相同的任务能被纯decoder的transformer解决。

![img](images/1687328545650-a89a262f-a734-44f9-bd49-39bf4300b9a0.png)

###### 4.7.2. 总结

这是第一个纯transformer模型训练的任务，也就是说， 它被训练成去阅读维基百科文章(没有目录前的开头部分), 并去归纳总结它，文章实际开头的部分被用作训练数据的标签。

![img](images/1687328785962-50649b23-8989-4a11-88a6-35193612b4d9.png)

论文针对维基百科的文章对模型进行训练，因此训练后的模型能对文章进行总结。

![img](images/1687328943226-390986a7-284c-458f-b2b2-a8f34a5dcac0.png)

###### 4.7.3. 迁移学习

在 [Sample Efficient Text Summarization Using a Single Pre-Trained Transformer](https://arxiv.org/abs/1905.08836)这篇文章中，一个纯decoder的transformer模型是第一个预训练语言模型，然后微调去归纳。在有限额数据集上，它比预训练的encoder-decoder transformer能取得更好的结果。

GPT2的论文还展示了在对语言建模进行预训练后的总结结果。

###### 4.7.4. 生成音乐

[Music Transformer](https://magenta.tensorflow.org/music-transformer)使用纯decoder的transformer生成具有表现力的时间和动态音乐。音乐模型就像是语言模型——只是让模型以一种非监督的方式去学习音乐，然后对输出进行采样(就是我们之前说的"漫游")。

你可能会好奇，在这个场景中，音乐是如何表征的。请记住，语言建模可以通过字符、词或作为词的一部分的标记的向量表示来完成。

对于音乐表演（现在我们先考虑钢琴），我们必须表示音符，但也要表示速度--衡量钢琴键被按下的力度。

![img](images/1687331209592-9698eaf2-b9c9-45de-9589-df0e40fb1e77.png)

一场演出就是一系列这样的one-hot向量，一个midi文件能转换成这种格式，文章有以下输入序列的例子:

![img](images/1687331476633-787d5b99-82cf-4b9d-b26f-8032ecc95169.png)

one-hot向量代表的输入序列可能是这样的。

![img](images/1687331536751-cf6bcb47-7aee-4365-8ccd-b5ca6b63e89a.png)

我喜欢论文中的一个视觉效果，它展示了音乐transformer中的self-attention。我在这里给它加了一些注释：

![img](images/1687331700342-eb2b2c15-40d6-4e64-b23a-071032334e87.png)

"图8：这首曲子有一个反复出现的三角形轮廓。query是在后面的一个山峰上，它附和着山峰上所有以前的高音，一直到作品的开始"。... "[the]图显示了一个query（所有注意线的来源）和正在关注的以前的记忆（正在接受更多softmax probabiliy的音符被突出显示）。注意线的颜色对应于不同的头，宽度对应于softmax概率的权重"。

##### 4.8. 总结

这就结束了我们对GPT2的研究，以及对其母体模型--纯decoder transformer的探索。我希望你在读完这篇文章后，对自我关注有了更好的理解，并对你了解transformer内部发生的更多事情感到更加欣慰。



#### 5. BERT可视化

##### 5.1. 简介

在过去几年，处理语言的机器学习模型得到了快速发展。这一进展已经离开实验室，开始为一些领先的数字产品提供动力。这方面的一个很好的例子是，最近宣布BERT模型现在是谷歌搜索背后的主要力量【https://www.blog.google/products/search/search-language-understanding-bert/】。谷歌认为这一步（或应用于搜索的自然语言理解方面的进展）代表了 "过去五年中最大的飞跃，以及搜索历史上最大的飞跃之一"。

这篇文章是关于如何使用BERT进行句子分类的简单教程。这是一个例子，作为第一个介绍，足够基本，足够先进，可以展示其中的关键概念。

除了这篇文章，我准备了一个笔记本，你可以在这里查看。

##### 5.2. 数据集

我们在例子中使用的数据集是SST2【https://nlp.stanford.edu/sentiment/index.html】, SST2包含来自电影评论中的句子，每个句子的标签是0(表示正面)或1(表示负面)。

![img](https://cdn.nlark.com/yuque/0/2023/jpeg/32553153/1687680445178-eb545a0d-1f3d-4515-9a4d-df63ad965f8c.jpeg)

##### 5.3. 模型： 句子语义分类

我们的目标时创建一个模型，该模型针对每个句子(就像数据集中的那样)，要么输出1(表明句子携带正面语义)，要么输出0(表明句子携带负面语义)。我们可以把它看成这样。

![img](images/1687680739481-e898622a-6802-4387-ad71-d196b90b18bc.png)

在模型下面，该模型实际上由两个模型组成。

- DistillBERT: DistillBERT处理句子并将从中提取的信息传递给下一个模型。DistillBERT是更小版本的BERT，而且在HuggingFace中开源。它是BERT更轻更快的版本，与它的性能大致相当。
- 下一个模型，一个来源于scikit learn的基本Logistic Regression模型， 它接收DistilBERT模型的处理结果， 并将句子分为正面或负面(分别时1或0)

在两个模型中传递的数据是一个768维度的向量，我们可以把这个向量看作是我们可以用于分类的句子的embedding。

![img](images/1687681522431-c98bb8d1-fc55-45af-a373-36f08e1e51db.png)

如果你看过之前的图解BERT， 这个向量是第一个位置的结果(这个位置接收[CLS] token作为输入。

##### 5.4. 模型训练

我们会使用两个模型，我们只训练logistic回归模型。对于DistillBERT, 我们只使用已经预训练好的模型， 该模型对英语有一定的掌握。然而，这个模型既没有经过训练，也没有经过微调来做句子分类。然而，我们从BERT训练的一般目标中得到一些句子分类能力。这种情况在BERT的第一个位置（与[CLS]标记相关）的输出中尤为明显。我相信，这个由于BERT的第二个训练对象——下一个句子分类造成的。这一目标似乎是在训练模型将整个句子的意义囊括到输出的第一个位置上。transformers库提供了DistillBERT的一个实现以及它的预训练模型。

![img](images/1687682277530-764beb4e-3773-4b9c-904d-9c294d14e01a.png)

##### 5.5. 教程简介

因此，本教程的游戏计划是这样的。我们将首先使用经过训练的distilBERT来生成2000个句子的embedding。

![img](images/1687682411793-5b251a55-cc3e-4a83-b9b9-e6bd28c1c8ab.png)

在这一步之后，我们将不再碰distilBERT。从这里开始都是Scikit Learn。我们对这个数据集进行常规的训练/测试分割：

![img](images/1687682493343-c7211d9f-d11c-4228-8d34-c667080a68be.png)

对distilBert(模型#1）的输出进行训练/测试分割，得到的数据集用来训练和评估模型2中的logistics 回归。请注意，在实际中，sklearn的训练和测试切分前会对样本进行打乱， 而不是只全句子的前75%的数据。

然后在训练集上训练逻辑回归模型。

![img](images/1687682847412-316f6ca5-644e-48ef-bae2-022fb8bddbed.png)

##### 5.6. 单一的预测是如何计算的

在我们深挖模型代码以及解释如何训练模型前，可以先看看一个训练好的模型如何计算它的预测。

让我们尝试对"a visually stunning rumination on love”进行分类。第一步是使用BERT tokenizer将单词切分成tokens. 然后，我们会在第一个位置加上用于句子分类的token(第一个位置是[CLS], 句子结尾的是[SEP]).

![img](images/1687683251330-f73a0de8-015a-43f5-8838-29a2efefedc4.png)

tokenizer第三步是用embedding表中的id去代替每个toke，这个表示训练好的模型的一部分。可以看看图解word2vec来了解word embedding.

![img](images/1687683447021-b206d851-3fa4-4c40-960b-d5d833269710.png)

注意tokenizer只需要一行代码就可以完成这些步骤。

```python
tokenizer.encode("a visually stunning rumination on love", add_special_tokens=True)
```

输入的句子现在可以传递给DistilBERT。

如果你看过《图解BERT》， 这一步骤也可用这种方式来可视化。

![img](images/1687683880166-f52a39a2-78d3-40c8-b8a0-bfe38c9364d6.png)

##### 5.7. 流经DistilBERT

通过DistilBERT传递输入向量的工作方式与BERT一样，对每个输入token输出将会是一个向量， 每个向量包含768个数字(浮点型).

![img](images/1687684108758-d16e3ce3-3f29-4adb-bd16-47abd8327911.png)

因为这是一个句子分类任务，我们忽略了除第一个向量(该向量与[CLS]相关)之外的所有向量, 这个向量将作为逻辑回归的输入。

![img](images/1687684319965-74071c42-73dc-46bc-923a-ceab211a8448.png)

从这里开始，逻辑回归模型的工作就是根据它在训练阶段学到的东西对这个向量进行分类，我们可以把这个预测看成这样的。

![img](images/1687685222295-915b9dad-f778-4811-843c-38809fc31e20.png)

训练过程以及整个过程的代码会在下一部分讨论。

##### 5.8. 代码

在这部分，我们会强调句子分类模型的代码。

```python
import numpy as np
import pandas as pd
import torch
import transformers as ppb # pytorch transformers
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
```

数据集在这里【 [available](https://github.com/clairett/pytorch-sentiment-classification/)】

```python
df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\t', header=None)
```

使用df.head查看数据集

```python
df.head()
```

![img](images/1687685462738-1abb428b-f8a6-496d-97bd-8225eec843c2.png)

##### 5.9. 导入预训练的DistilBERT模型和tokenizer

```python
model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')

## Want BERT instead of distilBERT? Uncomment the following line:
#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')

# Load pretrained model/tokenizer
tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
model = model_class.from_pretrained(pretrained_weights)
```

我们可以对数据集tokenize化处理，注意我们现在做的与上面的例子有点点不一样。上面例子中只tokenize和处理一个句子。这里我们会以一个批次处理所有的句子(出于资源方面的考虑，笔记本会处理一组较小的例子，比方说2000个例子).

##### 5.10. Tokenization

```python
tokenized = df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))
```

这将每个句子转成一个ids列表。

![img](images/1687686011967-7d640943-dceb-49ce-bbdc-c95e267d8511.png)

数据集是一组列表。在DistilBERT将其处理为输入前，我们需要对较短的句子用标记0填充，使所有的向量具有相同的大小。

填充后，我们有一个准备好的矩阵/张量，可以传递给BERT：

![img](images/1687686289315-263d02e9-c091-42d0-8d44-a6870ef43f48.png)

##### 5.11. 用DistilBERT处理

我们从填充好的token矩阵中创建输入向量，然后将其传递给DistilBERT

```python
input_ids = torch.tensor(np.array(padded))

with torch.no_grad():
    last_hidden_states = model(input_ids)
```

完成这个步骤后，last_hidden_states持有DistilBERT的输出。它是一个元组，其形状为（例子的数量，序列中的最大标记数，DistilBERT模型中隐藏单元的数量）。在我们的案例中，它的形状为2000(我们只限制了2000个例子)， 66(2000个例子中最长的序列)， 768(DistilBERT模型的隐藏单元个数)。

![img](images/1687686855723-1c539b7c-216c-4eb5-b40c-7182edd326ba.png)

##### 5.12. Unpacking BERT的输出向量

现在解开3-d输出向量，首先从检查它的维度开始。

![img](images/1687687017032-a42891b3-4567-4c52-9aaf-3abff867a928.png)

##### 5.13. Recapping a sentence's journey回顾一个句子的历程

每一行都与我们数据集中的一个句子相关联。去回顾第一句子的处理路径，我们可以认为它看起来是这样的：

![img](images/1687742298364-d3b56276-6947-4397-8755-900bc8d05bd1.png)

##### 5.14. Slicing the import part  切分重要的部分

对于句子分类，我们只对BERT的[CLS] token的输出感兴趣，所以我们选择立方体的一个切片而放弃其他的东西。

![img](images/1687742613376-73f135c5-3b91-45ec-913e-128fe9e2e8d6.png)

这就是我们如何将三维张量切片，以得到我们感兴趣的二维张量：

```python
 # Slice the output for the first position for all the sequences, take all hidden unit outputs
features = last_hidden_states[0][:,0,:].numpy()
```

现在，特征是一个2D的numpy数组，包含了我们数据集中所有句子的句子嵌入。

![img](images/1687744222000-e33547f2-cfd0-4fb3-9790-b8bd6997c5bf.png)

我们从BERT的输出中切出的张量。

##### 5.15. 逻辑回归的数据集 Dataset for Logistic Regression

现在已经有了BERT的输出，我们已经组装了我们需要去训练逻辑回归模型的数据。768列就是特征，以及我们从初始数据集中获得的标签。

![img](images/1687751366665-9040d852-76d6-4fe5-b791-5514ace25136.png)

我们用于训练逻辑回归模型的标注的数据集。特征是BERT中[CLS]token的输出，[CLS]token是我们从原始的图片中切分出来的。每一行与我们数据集中的一个句子对应，每一列与前馈神经网络中的隐藏单元的输出相关，这个前馈神经网络是位于Bert/DistilBERT模型的顶端。

完成传统的机器学习中的train/split划分后，我们可以声明我们的Logistic回归模型，并针对数据集进行训练。

```python
labels = df[1]
train_features, test_features, train_labels, test_labels = train_test_split(features, labels)
```

数据集划分

![img](images/1687751861411-e5fd4b51-ff54-4442-9bf5-757bf923e139.png)

在训练集上训练我们的逻辑回归模型

```python
lr_clf = LosigticRegression()
lr_clf.fit(train_features, train_labels)
```

完成模型训练，针对测试集进行打分

```python
lr_clf.score(test_features, test_labels)
```

模型达到了81%的准确率

#### 6. How GPT-3 work——可视化与模拟（Visualizations and Animations）

##### 6.1. 简介

科技界对GPT3的炒作甚嚣尘上。大语言模型(像GPT3)的能力让我们开始吃惊。虽然对大多数企业来说，放在客户面前的东西还不完全可靠。这些模型正显示出聪明的火花，肯定会加速自动化的进程和智能计算机系统的可能性。

让我们揭开GPT3神秘的光环看看它是如何训练和工作的。

训练的语言模型生成文本。

我们可以有选择的将一些文本作为输入传给它，这会影响输出。

输出从模型训练阶段的学习中产生，在训练阶段，它扫描了大量的文本。

![img](https://cdn.nlark.com/yuque/0/2023/gif/32553153/1687761671850-b8c68c1f-df2a-4cc4-bda4-92b0f46c5f9f.gif)

训练是将文本暴露在大量文本的过程。这个过程已完成。你现在看到的所有实验都来自于一个训练好的模型，一年估计需要花费355块GPU， 耗费460万美元。

![img](https://cdn.nlark.com/yuque/0/2023/gif/32553153/1687762025486-d1c55541-59c3-4e6f-b0fe-39901dfb5b8c.gif)

由3000亿个文本标记组成的数据集用来为模型生成训练实例，例如，这些都是由顶部的一个句子产生的三个训练例子。

你可以看到，你可以在所有的文本上滑动一个窗口，做出很多的例子。

![img](images/1687762337024-d0170856-c08a-43d0-a4b2-e924f13a03a1.png)

模型以一个例子来说明。我们只向它展示特征并要求它产生下一个单词。

模型的预测可能会错。预测时我们计算它的错误，更新模型以便一下次能做更好的预测。

重复上百万次。

![img](https://cdn.nlark.com/yuque/0/2023/gif/32553153/1687763814638-910cb44f-902b-41bf-81fc-fa114e8435c9.gif)

现在更细致的看一下相同的步骤

GPT3通常一次产生一个输出（现在假设一个token就是一个单词)

![img](https://cdn.nlark.com/yuque/0/2023/gif/32553153/1687768600291-25b9dc20-e2ee-44fd-bf57-9cc5da3dcd81.gif)

请注意： 这是描述GPT-3模型是如何工作的，而不是它的新颖之处(主要是大的离谱). 模型架构师transformer decoder模型。

GPT3非常大，它将从它训练中学到的东西编码为1750亿个数字(称为参数)。这些数字被用来计算每次运行时需要产生哪些令牌。

未训练的模型开始时是随机初始化参数，训练可以得到更好的预测。

![img](images/1687769016872-8e70ca5b-053a-4d6e-aa7e-207f28394da5.png)

这些数字是模型内部数百个矩阵中的一个。预测也是大量矩阵相乘。

为了弄明白这些参数如何分布和使用的，我们会打开模型然后深入探究。

GPT3有2048个tokens宽度，这就是上下文窗口，这意味着它有2048条轨道，沿着这些轨道处理令牌。

![img](https://cdn.nlark.com/yuque/0/2023/gif/32553153/1687769496809-4ed3b93a-cf54-4ca7-919d-f0ce01529197.gif)

沿着紫色的轨道前进。一个系统如何处理单词"robotics"和"A"呢

高层次步骤：

- \1. 将单词转为向量
- \2. 计算预测
- \3. 将结果向量转为单词

![img](https://cdn.nlark.com/yuque/0/2023/gif/32553153/1687770002009-e9bd3dcb-0d7b-4e12-b9ee-497c3264fda8.gif)



GPT3中的重要计算发生在96个transformer decoder层堆叠的内部。

看这些层， 这就是"深度学习"中的深度。

每一个层有180万参数参与计算，这就是神奇之处。这是高层次视角观看的处理过程。

![img](https://cdn.nlark.com/yuque/0/2023/gif/32553153/1687770376369-cf4fc924-9a6a-4e70-a644-1c9bd131d72c.gif)

与GPT3不同的是密集和稀疏自注意力层交替出现。

这是输入的X-ray， 响应GPT3。注意每一个token都流经整个堆叠层，我们不关心前几个单词单词的输出。当输入完成后，我们开始关注它的输出。我们将每个单词反馈到模型中。

【图片太大了，传不上来， 在这里：E:\1_project\19_note\GPT-3\6_1_10_08-gpt3-tokens-transformer-blocks.gif 】



在代码生成案例中，描述可能是输入提示，再加上一些描述案例。我相信，反应代码会像粉红色标记一样，一个接一个的生成。

我的假设是，引言的例子和描述被附加为输入，用特定的标记来分隔例子和结果。然后送入模型。

【图片太大了，传不上来， 在这里：E:\1_project\19_note\GPT-3\6_1_11_09-gpt3-generating-react-code-example.gif】

这样的工作让人印象深刻。因为你只要等待GPT3的微调推出就可以了。可能性将更加惊人。

微调实际上是更新模型的权重让模型在特定任务中有更好的效果。

![img](https://cdn.nlark.com/yuque/0/2023/gif/32553153/1687773158632-6a032fa1-a595-4f38-a1d9-87d38c2775ae.gif)

#### 7. 解释性Transformer语言模型的接口

##### 7.1. 简介

通过观察输入显著性和神经元激活来探索transformer语言模型的接口。

![img](images/1687772362486-f193ffef-2fbb-40c5-98dd-40d1d6dbbdcb.png)

transformer架构一直在为NLP领域的最新进展提供动力。这里提供了架构的细目【图解transformer】。基于该架构的预训练语言模型， 无论是自动回归（模型使用自己的输出作为下一个时间的输入，像GPT2一样从左到右处理tokens）还是去噪变体（通过破坏或掩码输入训练模型，像BERT一样双向处理tokens）都在持续推动NLP中的多项任务的发展，最近在机器视觉领域。我们对模型效果这么好的理解始终落后于模型的发展。

本论述系列继续追寻解释和可视化基于transformer语言模型的内部工作原理。我们阐述了这些关键可解释性的方法如何应用于基于transformer的语言模型。本文关注自回归模型，但是这些方法同样适用于其他架构和任务。



这是这个序列的第一篇文章，在这篇文章中，我们介绍了可探索性和可视化，以便能直观的理解下面的这些概念：

- 输入显著性方法： 它针对输入tokens对产生一个token的重要性进行打分。
- 神经元激活以及单个和多个神经元模型如何对输入做响应并产生输出。

在可解释性机器学习文献的语言中，像Molnar【[Interpretable Machine Learning--A Brief History, State-of-the-Art and Challenges](https://arxiv.org/pdf/2010.09337.pdf)】等人。输入显著性是一种解释个体预测的方法。后面两种方法归属于"分析更复杂模型的组成部分"， 可更好的描述为增加transformer模型的透明度。

此外，这篇文章还附有可复现的笔记本和Ecco——一个开源库。可直接在Jupyter笔记本中为HuggingFace transformer中基于GPT的模型创建类似的交互库。

如果我们要把我们要研究的三个组件强加在transformer的架构上，它看起来就像下面的图。

![img](images/1687830577030-8cacf46b-8c50-4c02-b4be-df51310d86ef.png)

图: 有三种方法可以更深入地了解Transformer语言模型的内部运作。

通过引入可视化输入显著性、隐藏状态的演变和神经元激活的工具，我们旨在使研究人员能够建立关于Transformer语言模型的更多直觉。

##### 7.2. 输入显著性

当计算机视觉模型将一个图片分类为含有哈士奇的时候，显著性图能够告诉我们这个分类是否是由于这个动物本身的视觉特性， 或者是因为图片中的雪。这是一个解释模型输出和输入关系的归因方法——帮助我们检测错误和偏见，更好的理解系统的行为。

![img](https://cdn.nlark.com/yuque/0/2023/jpeg/32553153/1687831416749-5af2c241-4187-4192-8c5e-4bb591f07089.jpeg)

图:输入显著性图将模型的预测归于输入像素

存在多种为NLP模型的输入分配重要性分数的方法。文献中最长关注的是这种分类任务的应用， 而不是自然语言生成。本文重点关注语言生成。我们的第一个界面在每个标记生成后计算特征的重要性。通过悬停或点击输出标记，对负责生成该标记的标记施加一个显著性地图。

这个界面的第一个例子时要求GPT2-XL[5]提供莎士比亚的出生日期。该模型能够准确的生成日期(1564年，但是被分解成两个标记: 15和64， 因为模型的词汇表不包含"1564”这个单一的标记). 界面显示了每个输入标记在生成每个输出标记时的重要性。

![img](images/1687833139416-c013de38-8774-4f49-9e85-707d7eaa8230.png)

我们的第二个例子试图既探究模型的世界知识，又看模型是否重复文本中的模式（简单的模式如数字后的句号和像新行，以及稍微复杂的模式如完成一个编号的列表）。这里使用的模型是DistilGPT2[28]。



这个探索性文件显示了一个更详细的视图，它显示了每个标记的归属百分比 -- 以防你需要这种精确度。

![img](images/1687833250765-afdc7647-e565-4728-93f0-000213c5903a.png)

我们在本文的其余部分说明性地使用的另一个例子是，我们要求模型完成一个简单的模式：

![img](images/1687833499912-c6d382d6-c907-4c1f-99b1-de5861dc7a24.png)

也可以用这个界面来分析基于transformer的对话代理的反应。在下面的例子中，我们向DiabloGPT[29]提出了一个存在性问题：

![img](images/1687833573767-3ea3cb04-a9b0-4545-b6de-d8b3f927fc80.png)

##### 7.3. 基于梯度的显著性

上面展示的是基于梯度X输入的特征重要性评分——由Atanasova展示的基于梯度的显著性方法， 在不同的数据集的transformer模型的文本分类中表现最好。

为了说明如何工作的，我们首先回顾模型在每个时间步中如何生成输出token。在下面的图中，我们发现语言模型的最终隐藏状态投射到模型单词表中，从而导致模型单词表中每个token的数值分数。将这些分数向量输入到softmax算法会生成每个token的概率分数。基于我们的向量选择一个token(选择概率最高的得分token，或从得分前几名的tokens中抽样)

![img](images/1687834269924-4734c31b-2965-433e-b408-6cf80785c60e.png)

图: 基于梯度的输入显著性

通过计算选定的logit（在softmax之前）相对于输入的梯度，并将其一直反向传播到输入标记，我们可以得到一个信号，即每个标记在计算中的重要性，从而生成这个标记。这一假设是基于这样的想法：具有最高特征重要性值的输入标记的最小变化会对模型的结果输出产生很大的变化。

![img](images/1687834611162-09663d49-9bf0-4926-8eae-e507749a54dc.png)

图： 梯度X输入计算和汇总

每个token的梯度向量与对应输入embedding的相乘， 对结果向量取L2正则化得到token的特征重要性分数。将分数除以分数总和得到正则化分数。

更加正式的，梯度x输入可以描述为如下形式： 

![img](https://cdn.nlark.com/yuque/__latex/7d19b222286f6b405f8d52fa5727e5f1.svg)
![img](images/1687834901307-d361f3cb-5fc0-4a7f-b84e-b172ea067165.png)

Xi是时间步上的输入token的embedding向量。![img](images/1687834979319-d6fa8dd0-03ad-44c8-a705-e3d830ec0715.png)是选中token的分数的反向传播梯度， 展开如下所示：

- X1:n是输入序列(长度为n）中输入token的嵌入向量的列表。
- fc(X1:n):  是经过模型的前向传递（通过包括贪婪/argmax解码、抽样或波束搜索在内的任何一种方法选择）后，所选标记的得分。c代表 "类"，因为这通常是在分类背景下描述的。尽管在我们的案例中，"token "更合适，但我们还是保留了这个符号。

除了梯度和输入向量被逐元相乘外，这一形式化是由Bastings等人[32]所述。然后通过计算L2准则将得到的向量汇总成一个分数，因为阿塔纳索瓦等人[14]的经验表明这比其他方法（如平均法）表现更好。

##### 7.4. 神经元激活

前向反馈神经网络子层是transformer模块中两个重要模块中的一个(另外一个是self-attention)。它占据了transformer模块66%的参数，提供模型表征能力的有意义的部分。以前的工作已经研究了NLP和计算机视觉领域的深度神经网络内部的神经元启动情况。在本节中，我们将这一研究应用于基于transformer的语言模型。

###### 7.4.1. 继续数数： Continue Counting:1, 2, 3,__

为了指导我们的神经元检查，让我们用 "1，2，3 "的输入来展示我们的模型，希望它能呼应逗号/数字的改变，但同时也保持数字的递增。

它做到了：

![img](images/1687836747741-c18a4958-7d6b-4d34-b91d-3d67f705e2ca.png)

通过使用我们在文章2中使用的方法，我们能够产生一个图，这个图展示了模型中每层后的输出token的概率。他看的是每一层之后的隐藏状态，并显示该层中最终产生的输出标记的排名。

例如， 在第一步， 模型生成token “4”，第一列告诉我们这个过程。这一栏中最下面的单元格显示，在最后一层之后，token "4 "的概率排在第一位。意味着最后一层给了它最高的概率分数。上面的单元格表明token "4"在每一层后的排名。

通过观察隐藏状态，我们观察到在不同层中模型收集了输出序列的两个模式(列和升序的数)的置信度。

![img](images/1687837620599-e264397d-414a-4e1e-9466-e0d6f2b03d0f.png)

Layer4层发生了什么让模型将数字(4, 5, 6)都取得了最高的概率分布。

我们可以画出layer 4中的神经元激活以此来感受神经元的活动。这是在下面的三个图中的第一个图展示的。

![img](images/1687837890644-2eaf17d8-bcbb-4216-8aba-691e44b0c8de.png)第4层的FFN中200个神经元（共3072个）的激活情况，导致模型输出标记 "4"。

每一行都是一个神经元。只有具有正向激活的神经元被着色。它们的颜色越深，发射的强度越大。

![img](images/1687837897548-db62c4de-2959-4236-9d24-7c49462f89ca.png)第4层FFN子层中的神经元启动情况

每一行对应于第4层前馈神经网络中的一个神经元。每一列是该神经元在产生一个令牌时的状态（即图中顶部的令牌）。

前400个神经元的视图显示了激活通常是多么稀疏（在DistilGPT2中FFN层的3072个神经元中）。![img](images/1687837906688-9457dc3f-5665-4044-96fc-cf9b170bd55a.png)

通过激活值对神经元进行聚类

为了定位信号，对神经元进行聚类（对激活值使用kmeans），以揭示发射模式。我们注意到：

最大的一两个集群往往是稀疏的，如橙色的集群。

绿色集群中的神经元在生成数字时发射最多。

然而，红色集群中的神经元在生成逗号时发射最多。

紫色集群跟踪数字，但强度较小，神经元的数量较多。

粉红色集群中的神经元集中在数字上，在产生逗号时很少发射。它们的激活度越高，令牌值的递增就越高。

如果可视化和检查正确，神经元发射可以揭示单个神经元和神经元组所能发挥的互补和组成作用。

即使在聚类之后，直接看激活也是一件粗略和嘈杂的事情，正如Olah等人介绍的，我们可使用矩阵分解方法来降低维度。我们遵循作者的建议，使用非负矩阵分解(NMF)作为降低维度的自然候选者，将其分为可能单独更容易解释的组。我们的第一个实验是PCA， 但是NMF是一个更好的方法，因为它难以解释在神经元启动中的PCA组件的负值。

##### 7.5. Factor Analysis

通过首先捕捉模型中FFNN层的神经元激活，然后使用NMF将他们分解为更容易管理的因子数值，我们能够阐明不同神经元中如何对每个生成的token做贡献。

最简单的方法是将激活分为两个因素。在我们下一次的交互中，我们让模型产生30个token，将激活分解为两个因素， 并以生成该标记时激活度最高的因素来突出每个token。

![img](images/1687851050410-3f190086-2266-44a9-a536-d0a97b32eb40.png)

这个交互能够压缩很多数据，展示由神经元组成的因素的兴奋程度。左边的火花线给出了整个序列中每个因子的兴奋程度的快照。与火花线的交互(通过鼠标悬停或屏幕点击)会在右边序列的token中展示因子的激活。

我们发现将activations分解为两个因素， 将激活分解为两个因素的结果是与我们正在分析的交替模式(逗号, 和升序数字）相对应的因素。我们可以增加因子的数量量提高因子分析的分辨率，下面的图将同一个激活分解为5个因子。

![img](images/1687852070307-1ce8fa17-cc73-44fc-a041-41daf3114cce.png)

我们开始将这个扩展到带有更多上下文的输入序列中， 就像欧洲国家列表一样。

![img](images/1687852176347-a1071e33-440e-4a50-9171-b2ab495de5ae.png)

另外一个例子是DistilGPT2对SML的反应，显示了对语法的不同组成部分的关注因素的明确区分。 这次将激活分解成十个组成部分。

![img](images/1687852423266-21f2af48-db19-4a59-bd06-5217715f3bcc.png)

![img](images/1687852433618-7de9f3bf-60ba-4b62-be0c-32473c529446.png)

##### 7.6. 单层激活的因子化

这个界面是隐藏状态检查的好伙伴，它可以突出特定的兴趣层，利用这个界面我们可以把分析的重点放在兴趣层上。将这种方法应用于感兴趣的特定层是非常简单的。隐藏状态演变图表明layer #0做了很多繁重的工作， 因为它倾向于将进入概率分布顶部的token列入短名单。

下图展示了对费奥多尔-陀思妥耶夫斯基的一段话，应用于第0层的激活的十个因素：

![img](images/1687853498404-4009a6e0-c07b-4acb-9445-d254887c9b2c.png)

![img](images/1687853516943-63826f9e-7a81-4a67-87f5-650b059f082c.png)

我们可以通过增加因子的数量来提高分辨率。增加到18个因素，就可以揭示出对副词有反应的因素，以及对部分标记有反应的其他因素。再增加因素的数量，你就会开始识别出对特定的词语（"没有什么 "和 "人 "似乎对该层特别有吸引力）有反应的因素。

##### 7.7. 关于激活因子分析，About Activation Factor Analysis

上面的探索结果显示了使用非负矩阵分解法对持有FFN神经元激活值的矩阵进行分解所产生的因素，下图说明了这是如何做到的。

![img](images/1687854255727-7b47a09b-e9e4-41ca-b3c2-2303896b223f.png)

除了降维， 非负矩阵分解能揭示一组神经元的潜在常规行为。它可用于分析整个网络，单一层或一组层。





#### 8. 语言模型隐藏状态可视化：Hidden State Visualizations for Language Models

通过可视化模型层之间的隐藏状态， 我们能够得到一些模型"思维过程"的线索。

![img](images/1687854859405-2a6fd011-0d78-47b3-b4cc-81dd6217a582.png)

图: 当一个语言模型生成一个句子时，我们能看到模型如何产生每个字。这个值和颜色表明那一层输出token的排名。颜色越深， 排名越高。Layer0在最高处，Layer47在底部

part2: 继续追求使transformer语言模型更透明，这篇文章展示了一系列的可视化资料， 以揭示预训练的语言模型中的语言生成机制。这些可视化是由Ecco创建的。



本系列的第一部分，[Interfaces for Explaining Transformer Language Models](http://jalammar.github.io/explaining-transformers/), 我们展示了输入显著性和神经元激活的交互界面。 在这篇文章中， 我们会关注隐藏状态， 因为它从模型层演变到下一模型层。通过查看由transformer decoder模块产生的隐藏状态，我们的目的是获取有关语言模型如何到达特定输出标记的信息。Voita等人[1]探讨了这种方法。Nostalgebraist[2]提出了令人信服的视觉处理方法，展示了通过模型的各个层来演化隐藏状态的标记排名、logit得分和softmax概率的演变。

【以上未整理完，感觉是别人论文里面的东西，关注的是非常小的点】



#### 9. 图解检索transformer 【The Illustrated Retrieval Transformer】

##### 9.1. 简介

summary：最新的一批语言模型可以小得多，但通过能够查询数据库或搜索网络信息，达到类似GPT-3的性能。一个关键的迹象是：建立一个越来越大的模型并不是提升性能的唯一途径。

过去的几年见证了大语言模型的崛起——能够快速提升机器处理能力并生成语言的机器学习模型。自2017年以来的两点包括：

- 原先的transformer打破了之前的机器翻译的记录
- BERT让预训练和微调过程以及基于transformer的上下文单词embedding变得流行。它快速为Google搜索和Bing搜索助力。
- 开始的T5到后来的T0进一步推动力迁移学习的边界(在一个任务上训练， 然后让其在一个临近的任务上训练)将不同的任务设定为文本到文本的任务)。
-  GPT-3表明， 生成式模型大规模扩展可带来令人震惊的新兴应用(工业上继续训练大模型，像Gopher，MT-NLG...)。

一时之间， 似乎表明不断扩大模型是提升性能的唯一途径。最近在这方面的发展，像RETRO Transformer和OpenAI的WebGPT似乎扭转的这一趋势，这些模型表明如果我们用一种搜索/查询信息的方法来增强它们，更小的生成式语言模型性能就能媲美一个大模型。

这篇文章打破了DeepMind的RETRO(Retrieval-Enhanced Transformer）以及它的工作原理。这个模型性能与GPT-3类似， 但它只有GPT-3 4%的大小(750万的参数，而GPT3有1.85亿参数)

![img](images/1687918395596-7def2232-5f8c-4f8a-85f9-36b702ce98e1.png)

RETRO集合了从数据库中检索的信息， 将其参数从昂贵的事实和世界知识存储中解放出来。

RETRO在[Improving Language Models by Retrieving from Trillions of Tokens](https://arxiv.org/abs/2112.04426).中提出。它延续并发展了研究社区的各种检索工作。



##### 9.2. Why this is important: 从世界知识中分离出语言信息

语言建模训练模型预测下一个单词，填充句子中空白。 本质上，填充空白有时需要事实信息的知识。例如：

![img](images/1687920274094-0ae41fd3-f782-4d2a-9dd1-8b06996ec776.png)

Input prompt: The Dune film was released in ....

其他时候，对语言的熟悉程度足以让人猜出空白处的内容。比如说：

![img](images/1687920374148-4f49ca8e-5e9d-4624-97fe-fcce9a4a2546.png)

Input prompt: its popularity spread by word-of-mouth to allow Herbert to start working full ....

这个区别是重要的，因为大语言模型将他们模型参数中他们知道的一起都进行编码。虽然这对语言信息来说是有意义的，但对事实和世界知识信息来说却是低效的。

通过在语言模型中引入检索方法，模型可以更小。一个神经数据库帮助它检索文本生成过程中所需的事实信息。

![img](images/1687920719110-a20d2617-52b9-492c-95c5-0850d629fc1f.png)

通过检索方法辅助语言模型，　可以减少一个语言模型需要在其他参数中编码的信息量，以便在文本生成中表现良好。

小的语言模型训练更快，　因为训练数据的内存减少了。每个人都能将模型部署在更小更便宜的GPU上，并根据需要进行调整。

机械上，RETRO是一个encoder-decoder模型，就像最初的transformer模型。然而，在检索数据库的帮助下增加了输入序列。模型在数据库中查找更可能的序列然后将它加入到输入中。RETRO神奇般的生成输出预测。

![img](images/1687921267307-c70adf29-f576-410e-822f-7e30c37005cf.png)

RETOR使用数据库增强输入提示，这个提示用来从数据库中检索相关的信息。

在我们探索模型架构前，让我们深入了解一下检索数据库。

##### 9.3. 探索RETRO检索数据库 Inspecting RETRO's Retrieval Database

数据库按键值对存储

键是标准BERT中句子embedding

值包含两部分：

- 邻居：  用于计算它的键
- 完成度。 原始文件中文本的延续

RETRO数据库中包含2千万个基于Massive Text dataset的多语言token。neighbor和完成度块最大长度为64个tokens.

![img](images/1687922381431-7abd53cf-d8df-4610-a344-6220a2827b53.png)

图：观察RETRO数据库， 它展示的是RETRO数据库中的键值对案例。值包括邻居块和完成块。

RETRO将输入提示分解为多个块。为了简单起见，我们将集中讨论一个块是如何用检索到的文本进行扩充的。然而，该模型对输入提示中的每一个块（除了第一个）都做了这个过程。

##### 9.4. 数据库查找

在点击RETRO之前，输入提示进入BERT。然后对输出的上下文向量进行平均化，以构建一个句子嵌入向量。然后，该向量被用于查询数据库。

![img](images/1687922960999-519e5cf2-c99d-456d-9d56-cc8587c655e1.png)

用BERT处理输入提示产生上下文的标记嵌入。对它们进行平均处理，产生一个句子嵌入。

这个句子embedding用于最近的邻居搜索

检索两个最近的邻居，它们的文本作为RETRO的输入的一部分。

![img](images/1687923315272-618de0d2-cfda-4812-8e1e-ca7a9e3e6677.png)

BERT的句子embedding用于从RETRO的神经数据库中检索最近的邻居。然后将它们添加到语言模型的输出中。

这是RETRO的输入。输入提示和它的来自数据库的最近邻居。

从这里开始，transformer和RETRO模块将信息纳入到它们的处理中。

![img](images/1687923841993-d793da7f-122e-4f21-acc3-01fb58ea84fc.png)

##### 9.5. 高层次上RETRO的架构  RETRO Architecture at a High Level

RETRO架构是一个encoder堆叠和一个decoder堆叠

![img](images/1687923835225-4bc8ba32-5a99-471a-bacc-df99e8f5e5dd.png)

encoder是由标准的transformer encoder模块组成(self-attention + FFNN)。 据我理解，Retro使用了一个由两个transformer encoder组成的encoder。

decoder堆叠了两种decoder模块

- 标准的transformer decoder模块(ATTN + FFNN）
- RETRO decoder模块(ATTN + Chunked cross attention(CCA) + FFNN)

![img](images/1687924173750-a62b4965-78f8-40b3-8fa2-a7d68cdb8150.png)

图: 三种类型的transformer 模块组成RETRO

现在来看一下encoder堆叠模块，该模块处理检索邻居，得到KEYS和VALUES矩阵，然后用于attention【可参考图解transformer】

![img](images/1687924341324-b1059388-fa08-40ab-8f1e-bcd7882304fe.png)

图: encoder堆叠处理检索邻居得到keys和values矩阵

就像GPT一样，Decoder模块处理输入文本，它在提示token上使用self-attention， 然后传送到FFNN层

![img](images/1687924583151-66e3376f-6903-4b3c-a196-0223f4e5e06c.png)

只有当RETRO decoder到达时，我们开始纳入检索的信息。从第9个模块中每3个模块是一个 RETRO 模块(这允许它的输入到邻居)。 所以第9、12、15...32是RETRO模块(这两个小的Retro 模型，以及这个Retrofit 模型从第6层而不是第9层开始有这些层)

![img](images/1687933082045-86c32df9-8289-482f-abeb-5bf6ee8fb371.png)

图:　输入提示到达RETRO层开始加入检索信息

因此，有效地，这一步是检索的信息可以瞥见它需要完成提示的日期。

![img](images/1687933224569-33dad237-49b2-4e89-bd9d-b4288d91ce60.png)

RETRO decoder 模块使用Cross-Attention块从最近的邻居块中检索信息。



#### 10. 在现实世界中使用大语言模型【Applying massive language models in the real world with Cohere】

不到一年前，我参与了神奇的Cohere团队，这个公司训练了大预言模型(既像GPT又像BERT)，并提供给它们作为API使用(API支持微调)。创始人包括谷歌云脑以及transformer论文的联合作者。它是一个非常吸引人的角色，帮助公司或开发者使用大语言模型解决现实中的问题。



#### 11. 图解稳定扩散 Illustrate Stable Diffusion

##### 11.1. 简介

AI图像生成式最近最能抓住人们内心的AI能力。从文本描述中创建绚丽的视觉能力有神奇的魔力， 并明确指出人类艺术创作方式的改变。

Stable Diffusion的发行驶发展中的一个清洗的里程碑， 因为它让高性能模型面向大众。（性能指的是图像质量，速度以及相对较低的资源和内存需求)。

完成AI图像生成实验后，你可能会想它是如何工作的，

这是一个stable diffusion的温和介绍。

![img](images/1687935195769-48e02294-d5f3-4c90-9fc0-a5fe7107870a.png)

stable diffusion多才多艺，人们可以有很多不同的方法使用它。我们首先关注从文本到图像的生成(text2img)。上面的图展示的例子时：输入文本生成图像（实际完成的提示是这样的）. 除了文本到图像，另外一个就是让它改变图像(所以输入是text+image).

![img](images/1687935487898-d4f5a907-441b-4b26-ae28-9e8f310a229a.png)

让我们开始探索模型的内部，因为它可以解释它的组成部分，它们是如何交互的，以及图像生成参数的含义。

##### 11.2. stable diffusion的组成

stable diffusion是一个由不同组件和模型组成的系统，它不是一个单一的模型。

当我们探索模型内部时， 我们首选观察的是文本理解模块， 该模块将文本信息翻译成能够捕捉文本含义的数据表征。

![img](images/1687935744576-f622ecc4-f197-4c4b-8698-cb63a10a80d4.png)

我们从高维的视角来看，本文的后面我们会进入更多机器学习的细节。然而，我们可以说文本decoder是一个特殊的transformer语言模型(技术上: 文本CLIP模型的文本encoder)。输入文本，输出每个文本中每个字的数字表征。(每个token是一个向量)。

这些信息然后传递到图像生成器，图像生成器本身包含很多组件。

![img](images/1687935979239-decc9c52-2204-4465-984c-990b294b10d2.png)

图像生成器经历两个步骤：

- \1. 图像信息创建

该组件是stable diffusion的秘方。与以前的模型相比，大量的性能提升都是在这里实现的。

该组件运行多个步骤去生成图像信息。这是stable diffusion界面和库中的步骤参数，通常默认为50或100。

图像信息创造器在图像信息空间中完成。在文章的后面我们会谈到它的含义。这个特性让它在像素空间上比之前的diffusion 模型更快。在技术项目上，该组件由UNet神经网络和调度算法组成。

"diffusion"这个字描述了组件内部发生了什么。这是一步一步信息处理的过程，最终会生成高质量图像。

![img](images/1687936624403-a2d4207f-509b-4dc6-b7ec-502aa7b13941.png)

##### 11.3. 图像解码 Image Decoder

图像解码器从信息创建器重得到的信息创作图像。在处理过程的最后只需运行一次就可以生成最终的像素图像。

![img](images/1687936898730-8fd14a0d-4ef1-4254-a3aa-b627980dfb88.png)

我们可以看到三个主要的组件(每个都有它自己的神经网络)组成stable diffusion

- 为文本编码剪辑文本

输入： text

输出: 77个token embeddings向量，每个76维

- UNet+scheduler: 在信息空间处理或扩散信息

输入： text embedding以及多维度向量组成的噪音(结构化的数字列表，也称为向量).

输出: 处理完毕的信息数组

- 自动编码 解码器：使用处理完成的信息数组完成最终图像的绘制

输入：处理完成的信息数组

输出： 最终的图像(维度:(3, 512, 512）,分别代表(红/绿/蓝， 宽， 高))

![img](images/1687938581216-ea4802bd-db31-4fcb-a301-6e62d411310f.png)

##### 11.4. 到底什么是扩散， what is Diffusion Anyway

扩散是发生在粉红色 "图像信息创造者 "组件内部的过程。有了代表输入text的token embedding和一个随意初始化的图像信息数组(也叫潜在的)， 这个过程生成信息数组， 图像decoder使用它来完成最终图像的绘制。

![img](images/1687938894863-bd333069-fcd3-44ca-922e-e44d3357c995.png)

这个过程是按部就班的进行的，每一步会添加更相关的信息。为了让这个过程更直观，我们看一下随机潜在数组， 并看到它转成视觉噪音。在这种情况下， 视觉检查是通过图像decoder进行的。

![img](images/1687939184564-ed36fe2c-9879-4b9d-afb5-b1e81551dd34.png)

扩散发生在多个步骤中，每个步骤都对输入的潜标阵列进行操作，并产生另一个潜标阵列，这个潜标阵列更像输入的文本和模型从模型训练的所有图像中获取的所有视觉信息。

![img](images/1687939452876-e36dd5d6-4acc-4683-9b06-194379af36c9.png)

我们可以可视化一组潜在数组， 去看看每一步有哪些信息被添加。

![img](images/1687939622977-cabcf528-5e42-441d-bb1b-f8b2e82dd9ce.png)

这个过程令人叹为观止。

【图片在这里，E:\1_project\19_note\stable_diffusion\11_4_5_diffusion.gif】

在这种情况下，在第2步和第4步之间发生了一些特别迷人的事情。仿佛轮廓从噪音中浮现出来。

##### 11.5. diffusion是如何工作的， how diffusion works

利用diffusion模型产生图像的中心思想依赖于我们拥有强大的计算机视觉模型这个事实。提供大量的数据集，这个模型可以学习复杂的操作。 扩散模型通过将问题框定如下来处理图像生成：

假设我们有一张图片，我们生成一些噪音，并添加到图像中。

![img](images/1687940391320-b554bfb3-7439-453e-88d4-986f0d839ed8.png)

现在这个可以被认为是一张训练图像，我们可以使用公式去创建一些训练样本去训练我们图像生成模型的中心组件。

![img](images/1687940496077-1b8da01f-84da-46ec-9aaa-5c33fe9d2cdd.png)



虽然这个例子显示了从图像（0，无噪音）到总噪音（4，总噪音）的一些噪音值。 我们可以很容易控制添加多少噪音到图像中，所以我们将它分散到几十步， 为数据集中的所有图片中每张图片创建几十个训练样本。

![img](images/1687940732383-a7d45224-2ca5-4e0a-b2db-a75fba8354c8.png)

有了这个数据集，我们就可以训练噪声预测器，最终得到一个优秀的噪声预测器，当以某种配置运行时，它可以实际创造图像。如果你接触过ML，训练步骤看起来应该很熟悉：

![img](images/1687940926317-eaf1c085-2c3a-4d5d-83cd-f372e836590a.png)

现在来看一下这个是如何产生图像的。

##### 11.6. 移除噪音创建图像 Painting images by removing noise

训练有素的噪声预测器可以取一个有噪声的图像，以及去噪步骤的数量，并能够预测噪声的片断。

![img](images/1687941177234-08571ac5-8cb0-47a6-bb3f-a38848ba0699.png)

预测噪音样本，然后我们可以将噪音从图像中减去， 我们得到一个与模型训练的图像比较接近的图像(不是确切的图像本身，而是分布--在像素排列的世界里，天空通常是蓝色的，并高于地面，人们有两只眼睛，猫看起来有某种方式--尖耳朵，显然没有印象。）

![img](images/1687941375490-d920c756-b5cd-4b09-b716-7bb4df2c2f06.png)

如果训练图像是美学上的图像(stable diffusion在这个数据集上训练的， [LAION Aesthetics](https://laion.ai/blog/laion-aesthetics/))， 然后这个结果图像在美学上时具备美感的。如果我们在logo图像上训练，我们最终会得到logo生成模型。

![img](images/1687941644176-a5e91854-079d-48b4-a8b8-4fabfa90484e.png)

这就结束了主要在【[Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)】中描述的通过扩散模型生成图像的描述。现在你有了扩散的直观理解，你知道了stable diffusion， Dall-E 2 以及Google Imagen的主要组成部分。

请注意到目前为止我们描述的图像生成过程中没有使用任何文本数据。所以我们部署模型时， 它会产生好看的图像，但我们没有办法控制它， 比如图像可能会是金字塔，猫或者其他的什么。在下一步部分，为了能控制模型生成的图像， 我们会讨论文本如何纳入到这个过程中。 

##### 11.7. 速度提升： 在压缩(或潜伏)数据而不是在像素图像上的扩散

为了加速图像生成过程，稳定扩散论文不是在像素图像本身而是在图像的压缩版本上运行扩散过程。这个论文叫" Depatrture to Latent sapce"【[The paper](https://arxiv.org/abs/2112.10752) 】

压缩(以及后面的解压缩/绘制)是通过自编码器完成的。自编码器使用编码器将图像压缩到潜在空间， 然后解码器只用压缩的信息来重建它。

![img](images/1687942813912-d92d6d2b-c13d-4500-b099-1049ae10a92b.png)

现在前馈扩散过程在压缩潜伏上完成。噪音片段应用在这些潜伏上的，而不是像素图像上。噪声预测器实际上是被训练来预测压缩后的表示（潜像空间）中的噪声。

![img](images/1687943125966-1469eef4-298b-4baa-bb4c-e5c83a3ff08c.png)

前向过程(使用自编码的编码器)就是我们如何产生数据去训练噪音预测器。一旦它完成了训练， 我们就可以运行反向过程生成图像（使用自编码器中的解码器).

![img](images/1687943303639-312cb60e-8549-4d65-8192-67338f939dad.png)

这两个流程就是LDM/稳定扩散论文中的图3所示的内容：

![img](images/1687943544462-b3840bae-a49d-494d-88f2-6c6218367078.png)

该图还显示了 "条件 "组件，在本例中是描述模型应该生成什么图像的文本提示。因此，让我们来深入了解一下文本组件。

##### 11.8. 文本编码器： 一个transformer语言模型  The Text Encoder: A Transformer Language Model

transformer语言模型用于语言理解组件，该组件接受文本提示且能产生token embeddings. 发布的Stable Diffusion模型使用ClipText（基于GPT的模型), 而这篇文章使用的是BERT.

Imagen的论文显示，语言模型的选择是一个重要的选择。与较大的图像生成组件相比，交换较大的语言模型对生成的图像质量有更大的影响。

![img](images/1687943921763-114ae755-4b3e-40df-8287-ebba6743347b.png)

图:更大/更好的语言模型在图像生成模型的质量上有积极影响)

早期的Stable diffusion模型仅仅插入OpenAI发布的预训练ClipText模型。未来的模型可能切换到新的发布版本以及更大的CLIP变体。新的批次包含的文本模型有354M的参数，而ClipText有63M的参数。

##### 11.9. CLIP是如何训练的，How CLIP is trained

CLIP在图像以及图像说明数据集上训练的。想象一下这样的数据集，只有4亿图像和他们的说明。

![img](images/1687944342766-5dd23d38-7880-4d25-892e-bf7df45cdfa2.png)实际上，CLIP是在从网络上抓取的图片及其 "alt "标签上训练的。

CLIP是图像encoder和文本encoder的结合，它的训练过程可以简化输入一张图像和它对应的说明。我们对图像以及文本分别进行编码。

![img](images/1687944514159-7debe0e9-71a9-47df-a5ca-94fabfb2da73.png)

我们使用余弦相似度比较结果embeddings。当我们开始训练过程，相似度比较低，即使文本对图像的描述非常正确。

![img](images/1687944620956-b964f40f-68fa-4745-84da-3b9fe1271e39.png)

通过以大批量数据重复经过该数据集，我们得到了能够产生embedding的encoder，这个embedding中一张小狗的图片以及"a picture of a dog"这句话非常相似。就像Wrod2vec, 这个训练过程同样需要包含图像与说明不匹配的负样本， 模型需要分配它们较低的相似度分数。

##### 11.10. 将文本信息喂到图像生成过程。Feeding Text Information Into The Image Generation Process

为了让文本作为图像生成过程的一部分，我们必须调整噪声预测期使用文本作为输入。

![img](images/1687945070255-8029c85e-3dec-456a-801e-c80b0afc76e6.png)

我们的数据集现在包含encoded text. 因为我们正在潜伏空间处理它们， 输入图像和预测的噪音都在潜伏空间。

![img](images/1687945293152-23681655-55f8-43fe-ab25-3ba31ec09533.png)

为了更好的理解text tokens如何应用在Unet中，我们将深入探索Unet网络的内部。

##### 11.11. Unet 噪音预测层(没有文本) Layers of the Unet Noise predictor（without text）

首先看一下不包含文本的扩散Unet， 它的输入和输出可能是这样的。

![img](images/1687945511065-3fc7f304-8cb1-4bea-a0ff-a83779d7927e.png)

在内部，我看到：

- Unet是一系列层，致力于转化潜在数组
- 每一层在前一层的输出上操作
- 一些输出被送到网络的后期进行处理(通过残差连接)
- 时间步被转化成了一个时间步embedding向量，这就是在各层中使用的。

![img](images/1687945851937-1a9dd922-b6e9-4ea7-9f30-c79613497069.png)

##### 11.12. 带有文本的UNet噪音预测层 Layers of the Unet Noise predictor with text

现在让我们看看如何改变这个系统，以包括对文本的关注。

![img](images/1687945990373-92de416d-e4c9-492e-b8e1-f1d22e8a2d7e.png)

为了增加对文本输入的支持（技术术语：文本调节），我们需要对系统进行的主要改变是在ResNet块之间增加一个注意层。

![img](images/1687946079407-3fc6d11d-b21d-46d4-b865-3d7940787f70.png)

请注意，ResNet块并不直接看文本。但注意力层合并了潜意识中的那些文本表征。而现在，下一个ResNet可以在其处理过程中利用这些合并的文本信息。